# Resilience & Auto Scaling trÃªn AWS

## ğŸ“‹ Má»¥c lá»¥c

- [1. Giá»›i thiá»‡u](#1-giá»›i-thiá»‡u)
- [2. Auto Scaling trÃªn AWS](#2-auto-scaling-trÃªn-aws)
  - [2.1. ECS Auto Scaling](#21-ecs-auto-scaling)
  - [2.2. EKS Auto Scaling (Karpenter & HPA/VPA)](#22-eks-auto-scaling-karpenter--hpavpa)
  - [2.3. Lambda Concurrency & Scaling](#23-lambda-concurrency--scaling)
  - [2.4. So sÃ¡nh Auto Scaling: ECS vs EKS vs Lambda](#24-so-sÃ¡nh-auto-scaling-ecs-vs-eks-vs-lambda)
- [3. Multi-AZ & Multi-Region](#3-multi-az--multi-region)
  - [3.1. Multi-AZ â€” High Availability trong Region](#31-multi-az--high-availability-trong-region)
  - [3.2. Multi-Region â€” Disaster Recovery & Global Users](#32-multi-region--disaster-recovery--global-users)
  - [3.3. DR Strategies trÃªn AWS](#33-dr-strategies-trÃªn-aws)
- [4. Circuit Breaker & Resilience Patterns trÃªn AWS](#4-circuit-breaker--resilience-patterns-trÃªn-aws)
  - [4.1. Circuit Breaker vá»›i App Mesh (Envoy)](#41-circuit-breaker-vá»›i-app-mesh-envoy)
  - [4.2. Retry & Timeout trÃªn AWS](#42-retry--timeout-trÃªn-aws)
  - [4.3. Bulkhead trÃªn AWS](#43-bulkhead-trÃªn-aws)
  - [4.4. Rate Limiting trÃªn AWS](#44-rate-limiting-trÃªn-aws)
  - [4.5. Fallback Patterns trÃªn AWS](#45-fallback-patterns-trÃªn-aws)
- [5. Health Check & Self-Healing](#5-health-check--self-healing)
  - [5.1. Health Check trÃªn ECS](#51-health-check-trÃªn-ecs)
  - [5.2. Health Check trÃªn EKS](#52-health-check-trÃªn-eks)
  - [5.3. ALB/NLB Health Check](#53-albnlb-health-check)
  - [5.4. Route 53 Health Check](#54-route-53-health-check)
- [6. Chaos Engineering trÃªn AWS](#6-chaos-engineering-trÃªn-aws)
  - [6.1. AWS Fault Injection Service (FIS)](#61-aws-fault-injection-service-fis)
  - [6.2. CÃ¡c thÃ­ nghiá»‡m Chaos phá»• biáº¿n](#62-cÃ¡c-thÃ­-nghiá»‡m-chaos-phá»•-biáº¿n)
  - [6.3. Game Day â€” Quy trÃ¬nh thá»±c hÃ nh](#63-game-day--quy-trÃ¬nh-thá»±c-hÃ nh)
- [7. Disaster Recovery trÃªn AWS](#7-disaster-recovery-trÃªn-aws)
  - [7.1. Bá»‘n chiáº¿n lÆ°á»£c DR](#71-bá»‘n-chiáº¿n-lÆ°á»£c-dr)
  - [7.2. DR cho Microservice â€” Thiáº¿t káº¿ chi tiáº¿t](#72-dr-cho-microservice--thiáº¿t-káº¿-chi-tiáº¿t)
  - [7.3. Failover tá»± Ä‘á»™ng vá»›i Route 53](#73-failover-tá»±-Ä‘á»™ng-vá»›i-route-53)
- [8. VÃ­ dá»¥ thá»±c táº¿ â€” E-Commerce Resilience Architecture](#8-vÃ­-dá»¥-thá»±c-táº¿--e-commerce-resilience-architecture)
- [9. Anti-patterns](#9-anti-patterns)
- [10. Checklist triá»ƒn khai](#10-checklist-triá»ƒn-khai)
- [11. Tá»•ng káº¿t](#11-tá»•ng-káº¿t)
- [12. LiÃªn káº¿t liÃªn quan](#12-liÃªn-káº¿t-liÃªn-quan)

---

## 1. Giá»›i thiá»‡u

Trong [doc 10 â€” Resilience Patterns](10-resilience-patterns.md), chÃºng ta Ä‘Ã£ hiá»ƒu lÃ½ thuyáº¿t vá» Circuit Breaker, Retry, Bulkhead, Rate Limiter, Fallback, Health Check, Chaos Engineering. Doc nÃ y **Ã¡p dá»¥ng táº¥t cáº£ kiáº¿n thá»©c Ä‘Ã³ vÃ o thá»±c táº¿ trÃªn AWS** â€” mapping tá»«ng pattern lÃ½ thuyáº¿t sang AWS service cá»¥ thá»ƒ, káº¿t há»£p thÃªm **Auto Scaling** vÃ  **Disaster Recovery** â€” hai yáº¿u tá»‘ cá»‘t lÃµi cho há»‡ thá»‘ng production trÃªn cloud.

Doc nÃ y tráº£ lá»i cÃ¢u há»i: **Auto Scaling ECS/EKS/Lambda khÃ¡c nhau tháº¿ nÃ o? Multi-AZ/Multi-Region triá»ƒn khai ra sao? Circuit Breaker cáº¥u hÃ¬nh vá»›i App Mesh nhÆ° tháº¿ nÃ o? Chaos Engineering thá»±c hÃ nh trÃªn AWS báº±ng cÃ¡ch nÃ o? DR strategy nÃ o phÃ¹ há»£p?**

> ğŸ’¡ Giáº£ Ä‘á»‹nh: Báº¡n Ä‘Ã£ Ä‘á»c [doc 10](10-resilience-patterns.md) vÃ  hiá»ƒu lÃ½ thuyáº¿t. Doc nÃ y táº­p trung vÃ o **cÃ¡ch AWS hiá»‡n thá»±c hÃ³a** cÃ¡c khÃ¡i niá»‡m Ä‘Ã³.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          RESILIENCE & AUTO SCALING LANDSCAPE trÃªn AWS              â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€ Auto Scaling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ECS Service Auto Scaling    â† Target Tracking, Step        â”‚   â”‚
â”‚  â”‚  EKS HPA / VPA / Karpenter   â† Pod & Node scaling           â”‚   â”‚
â”‚  â”‚  Lambda Concurrency          â† tá»± Ä‘á»™ng, reserved/provisionedâ”‚   â”‚
â”‚  â”‚  Aurora Auto Scaling         â† Read Replicas                â”‚   â”‚
â”‚  â”‚  DynamoDB Auto Scaling       â† RCU/WCU                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€ Resilience Patterns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  App Mesh (Envoy)            â† Circuit Breaker, Retry      â”‚    â”‚
â”‚  â”‚  API Gateway                 â† Rate Limiting, Throttling   â”‚    â”‚
â”‚  â”‚  ALB/NLB                     â† Health Check, Failover      â”‚    â”‚
â”‚  â”‚  Route 53                    â† DNS Failover, Health Check  â”‚    â”‚
â”‚  â”‚  SQS Dead Letter Queue       â† Async error handling        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€ Disaster Recovery â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Multi-AZ                    â† High Availability (99.99%)  â”‚    â”‚
â”‚  â”‚  Multi-Region                â† DR, global users            â”‚    â”‚
â”‚  â”‚  Route 53 Failover           â† automatic DNS failover      â”‚    â”‚
â”‚  â”‚  S3 Cross-Region Replication â† data backup                 â”‚    â”‚
â”‚  â”‚  Aurora Global Database      â† cross-region replication    â”‚    â”‚
â”‚  â”‚  DynamoDB Global Tables      â† multi-region active-active  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€ Chaos Engineering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  AWS FIS                     â† managed chaos experiments   â”‚    â”‚
â”‚  â”‚  Game Day                    â† planned failure drills      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Auto Scaling trÃªn AWS

Auto Scaling lÃ  kháº£ nÄƒng **tá»± Ä‘á»™ng tÄƒng/giáº£m tÃ i nguyÃªn** dá»±a trÃªn workload thá»±c táº¿. TrÃªn AWS, má»—i compute platform (ECS, EKS, Lambda) cÃ³ cÆ¡ cháº¿ scaling riÃªng.

### 2.1. ECS Auto Scaling

ECS sá»­ dá»¥ng **Application Auto Scaling** Ä‘á»ƒ scale sá»‘ lÆ°á»£ng task (container) trong service.

#### CÃ¡c loáº¡i Scaling Policy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ECS AUTO SCALING POLICIES                  â”‚
â”‚                                                            â”‚
â”‚  1. Target Tracking Scaling                                â”‚
â”‚     â”œâ”€â”€ Giá»¯ metric á»Ÿ má»©c target (vÃ­ dá»¥: CPU = 70%)         â”‚
â”‚     â”œâ”€â”€ AWS tá»± Ä‘iá»u chá»‰nh desired count                    â”‚
â”‚     â””â”€â”€ ÄÆ¡n giáº£n nháº¥t, phÃ¹ há»£p háº§u háº¿t use cases           â”‚
â”‚                                                            â”‚
â”‚  2. Step Scaling                                           â”‚
â”‚     â”œâ”€â”€ TÄƒng/giáº£m theo tá»«ng báº­c (steps)                    â”‚
â”‚     â”œâ”€â”€ VÃ­ dá»¥: CPU 70-80% â†’ +2, CPU 80-90% â†’ +4            â”‚
â”‚     â””â”€â”€ Linh hoáº¡t hÆ¡n Target Tracking                      â”‚
â”‚                                                            â”‚
â”‚  3. Scheduled Scaling                                      â”‚
â”‚     â”œâ”€â”€ Scale theo lá»‹ch cá»‘ Ä‘á»‹nh                            â”‚
â”‚     â”œâ”€â”€ VÃ­ dá»¥: 8h sÃ¡ng â†’ min=10, 22h â†’ min=2               â”‚
â”‚     â””â”€â”€ PhÃ¹ há»£p workload cÃ³ pattern rÃµ rÃ ng                â”‚
â”‚                                                            â”‚
â”‚  4. Predictive Scaling (ECS trÃªn EC2)                      â”‚
â”‚     â”œâ”€â”€ ML dá»± Ä‘oÃ¡n traffic pattern                         â”‚
â”‚     â”œâ”€â”€ Pre-scale trÆ°á»›c khi traffic tÄƒng                   â”‚
â”‚     â””â”€â”€ Káº¿t há»£p vá»›i Target Tracking                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Cáº¥u hÃ¬nh ECS Auto Scaling (Terraform)

```hcl
# ECS Service
resource "aws_ecs_service" "order_service" {
  name            = "order-service"
  cluster         = aws_ecs_cluster.main.id
  task_definition = aws_ecs_task_definition.order.arn
  desired_count   = 3

  capacity_provider_strategy {
    capacity_provider = "FARGATE"
    weight            = 70
    base              = 2          # Minimum 2 tasks luÃ´n cháº¡y trÃªn Fargate
  }
  capacity_provider_strategy {
    capacity_provider = "FARGATE_SPOT"
    weight            = 30         # 30% tasks dÃ¹ng Spot (tiáº¿t kiá»‡m ~70% chi phÃ­)
  }
}

# Auto Scaling Target
resource "aws_appautoscaling_target" "order_service" {
  max_capacity       = 20
  min_capacity       = 2
  resource_id        = "service/${aws_ecs_cluster.main.name}/${aws_ecs_service.order_service.name}"
  scalable_dimension = "ecs:service:DesiredCount"
  service_namespace  = "ecs"
}

# Target Tracking â€” CPU
resource "aws_appautoscaling_policy" "cpu" {
  name               = "order-cpu-scaling"
  policy_type        = "TargetTrackingScaling"
  resource_id        = aws_appautoscaling_target.order_service.resource_id
  scalable_dimension = aws_appautoscaling_target.order_service.scalable_dimension
  service_namespace  = aws_appautoscaling_target.order_service.service_namespace

  target_tracking_scaling_policy_configuration {
    predefined_metric_specification {
      predefined_metric_type = "ECSServiceAverageCPUUtilization"
    }
    target_value       = 70.0
    scale_in_cooldown  = 300   # Chá» 5 phÃºt trÆ°á»›c khi scale in
    scale_out_cooldown = 60    # Scale out nhanh hÆ¡n (1 phÃºt)
  }
}

# Target Tracking â€” Custom Metric (SQS Queue Depth)
resource "aws_appautoscaling_policy" "sqs_backlog" {
  name               = "order-sqs-scaling"
  policy_type        = "TargetTrackingScaling"
  resource_id        = aws_appautoscaling_target.order_service.resource_id
  scalable_dimension = aws_appautoscaling_target.order_service.scalable_dimension
  service_namespace  = aws_appautoscaling_target.order_service.service_namespace

  target_tracking_scaling_policy_configuration {
    customized_metric_specification {
      metric_name = "BacklogPerTask"
      namespace   = "Custom/ECS"
      statistic   = "Average"
    }
    target_value = 100.0    # Má»—i task xá»­ lÃ½ tá»‘i Ä‘a 100 messages trong queue
  }
}

# Scheduled Scaling â€” Peak hours
resource "aws_appautoscaling_scheduled_action" "peak_hours" {
  name               = "order-peak-hours"
  resource_id        = aws_appautoscaling_target.order_service.resource_id
  scalable_dimension = aws_appautoscaling_target.order_service.scalable_dimension
  service_namespace  = aws_appautoscaling_target.order_service.service_namespace

  schedule = "cron(0 8 * * ? *)"    # 8h sÃ¡ng má»—i ngÃ y

  scalable_target_action {
    min_capacity = 5
    max_capacity = 30
  }
}
```

#### Metric nÃ o Ä‘á»ƒ scale ECS?

| Metric | Khi nÃ o dÃ¹ng | Target Value khuyáº¿n nghá»‹ |
|--------|-------------|------------------------|
| **CPU Utilization** | CPU-bound workloads (tÃ­nh toÃ¡n, image processing) | 60-80% |
| **Memory Utilization** | Memory-bound workloads (caching, in-memory data) | 70-85% |
| **ALB Request Count Per Target** | Web services, API endpoints | TÃ¹y capacity test |
| **SQS Backlog Per Task** | Queue consumers, async workers | Messages/task phÃ¹ há»£p throughput |
| **Custom CloudWatch Metric** | Business metrics (orders/min, active users) | TÃ¹y SLA |

> ğŸ’¡ **Best practice**: DÃ¹ng **Target Tracking trÃªn CPU** lÃ m baseline, káº¿t há»£p thÃªm **custom metric** cho business-specific scaling. Scale out nhanh (cooldown 60s), scale in cháº­m (cooldown 300s) Ä‘á»ƒ trÃ¡nh flapping.

### 2.2. EKS Auto Scaling (Karpenter & HPA/VPA)

EKS cÃ³ **ba táº§ng scaling** â€” Pod level, Node level, vÃ  Cluster level.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 EKS SCALING LAYERS                          â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€ Layer 1: Pod Scaling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  HPA (Horizontal Pod Autoscaler)                      â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Scale sá»‘ lÆ°á»£ng Pod dá»±a trÃªn metrics              â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ CPU, Memory, custom metrics (Prometheus)         â”‚  â”‚
â”‚  â”‚  â””â”€â”€ Giá»‘ng ECS Target Tracking                        â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  VPA (Vertical Pod Autoscaler)                        â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh CPU/Memory request/limit      â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ PhÃ¢n tÃ­ch usage â†’ recommend â†’ (optional) apply   â”‚  â”‚
â”‚  â”‚  â””â”€â”€ DÃ¹ng cho workloads khÃ³ horizontal scale          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â”‚                                 â”‚
â”‚                           â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€ Layer 2: Node Scaling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Karpenter (khuyáº¿n nghá»‹)                              â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Tá»± Ä‘á»™ng provision node khi Pod pending           â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Chá»n instance type tá»‘i Æ°u (right-sizing)         â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Mix On-Demand + Spot instances                   â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Consolidation â€” gom Pod, terminate node thá»«a     â”‚  â”‚
â”‚  â”‚  â””â”€â”€ Nhanh hÆ¡n Cluster Autoscaler (< 60s)             â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  Cluster Autoscaler (legacy)                          â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Scale ASG (Auto Scaling Group)                   â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Cháº­m hÆ¡n Karpenter (2-5 phÃºt)                    â”‚  â”‚
â”‚  â”‚  â””â”€â”€ Ãt flexible hÆ¡n vá» instance type                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### HPA â€” Horizontal Pod Autoscaler

```yaml
# HPA dá»±a trÃªn CPU + Custom Metric
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: order-service-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: order-service
  minReplicas: 3
  maxReplicas: 50
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30      # Scale up nhanh
      policies:
        - type: Percent
          value: 100                       # Tá»‘i Ä‘a gáº¥p Ä‘Ã´i
          periodSeconds: 60
        - type: Pods
          value: 5                         # Hoáº·c thÃªm 5 pods
          periodSeconds: 60
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300      # Scale down cháº­m (5 phÃºt)
      policies:
        - type: Percent
          value: 10                        # Giáº£m tá»‘i Ä‘a 10%
          periodSeconds: 60
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Pods
      pods:
        metric:
          name: http_requests_per_second    # Custom metric tá»« Prometheus
        target:
          type: AverageValue
          averageValue: "1000"              # 1000 RPS per pod
```

#### Karpenter â€” Node Auto Provisioning

```yaml
# Karpenter NodePool
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: default
spec:
  template:
    spec:
      requirements:
        - key: "karpenter.sh/capacity-type"
          operator: In
          values: ["on-demand", "spot"]     # Mix On-Demand + Spot
        - key: "node.kubernetes.io/instance-type"
          operator: In
          values:                           # Right-sizing: chá»n nhiá»u instance types
            - "m6i.large"
            - "m6i.xlarge"
            - "m6a.large"
            - "m6a.xlarge"
            - "m5.large"
            - "m5.xlarge"
            - "c6i.large"                   # Compute-optimized cho CPU-bound
            - "c6i.xlarge"
        - key: "topology.kubernetes.io/zone"
          operator: In
          values: ["ap-southeast-1a", "ap-southeast-1b", "ap-southeast-1c"]
      nodeClassRef:
        name: default
  limits:
    cpu: "200"                              # Tá»‘i Ä‘a 200 vCPU toÃ n cluster
    memory: "400Gi"
  disruption:
    consolidationPolicy: WhenUnderutilized  # Tá»± gom Pod, terminate node thá»«a
    expireAfter: 720h                       # Rotate node má»—i 30 ngÃ y
```

### 2.3. Lambda Concurrency & Scaling

Lambda cÃ³ cÆ¡ cháº¿ scaling **hoÃ n toÃ n tá»± Ä‘á»™ng** â€” má»—i request táº¡o 1 execution environment riÃªng.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               LAMBDA SCALING MODEL                         â”‚
â”‚                                                            â”‚
â”‚  Account Concurrency Limit = 1000 (default, cÃ³ thá»ƒ tÄƒng)   â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€ Unreserved Concurrency â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Chia sáº» giá»¯a táº¥t cáº£ Lambda functions              â”‚    â”‚
â”‚  â”‚  KhÃ´ng Ä‘áº£m báº£o capacity cho function cá»¥ thá»ƒ        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€ Reserved Concurrency â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Äáº·t trÆ°á»›c capacity cho function cá»¥ thá»ƒ            â”‚    â”‚
â”‚  â”‚  VÃ­ dá»¥: Payment function = 200 concurrent          â”‚    â”‚
â”‚  â”‚  âš ï¸ VÆ°á»£t limit â†’ throttle (429)                    â”‚    â”‚
â”‚  â”‚  Miá»…n phÃ­ â€” chá»‰ reserve, khÃ´ng charge thÃªm         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€ Provisioned Concurrency â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Pre-warm execution environments                   â”‚    â”‚
â”‚  â”‚  Loáº¡i bá» cold start hoÃ n toÃ n                      â”‚    â”‚
â”‚  â”‚  âš ï¸ Tá»‘n phÃ­ â€” charge theo provisioned units        â”‚    â”‚
â”‚  â”‚  DÃ¹ng cho: latency-sensitive (< 100ms required)    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                            â”‚
â”‚  Burst Limit:                                              â”‚
â”‚  â”œâ”€â”€ 3000 concurrent (us-east-1, us-west-2, eu-west-1)     â”‚
â”‚  â”œâ”€â”€ 1000 concurrent (cÃ¡c region khÃ¡c)                     â”‚
â”‚  â””â”€â”€ Sau burst: +500/minute cho Ä‘áº¿n account limit          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Cáº¥u hÃ¬nh Lambda Scaling (Terraform)

```hcl
# Lambda Function vá»›i Reserved Concurrency
resource "aws_lambda_function" "payment_processor" {
  function_name = "payment-processor"
  runtime       = "nodejs20.x"
  handler       = "index.handler"
  memory_size   = 512
  timeout       = 30

  reserved_concurrent_executions = 200    # Reserve 200 concurrent

  environment {
    variables = {
      PAYMENT_GATEWAY_URL = var.payment_gateway_url
    }
  }
}

# Provisioned Concurrency â€” loáº¡i bá» cold start
resource "aws_lambda_provisioned_concurrency_config" "payment" {
  function_name                  = aws_lambda_function.payment_processor.function_name
  provisioned_concurrent_executions = 50   # 50 pre-warmed instances
  qualifier                      = aws_lambda_alias.live.name
}

# Auto Scaling Provisioned Concurrency theo schedule
resource "aws_appautoscaling_target" "lambda_target" {
  max_capacity       = 200
  min_capacity       = 10
  resource_id        = "function:${aws_lambda_function.payment_processor.function_name}:${aws_lambda_alias.live.name}"
  scalable_dimension = "lambda:function:ProvisionedConcurrency"
  service_namespace  = "lambda"
}

resource "aws_appautoscaling_policy" "lambda_scaling" {
  name               = "payment-lambda-scaling"
  policy_type        = "TargetTrackingScaling"
  resource_id        = aws_appautoscaling_target.lambda_target.resource_id
  scalable_dimension = aws_appautoscaling_target.lambda_target.scalable_dimension
  service_namespace  = aws_appautoscaling_target.lambda_target.service_namespace

  target_tracking_scaling_policy_configuration {
    predefined_metric_specification {
      predefined_metric_type = "LambdaProvisionedConcurrencyUtilization"
    }
    target_value = 0.7    # Giá»¯ utilization á»Ÿ 70%
  }
}
```

### 2.4. So sÃ¡nh Auto Scaling: ECS vs EKS vs Lambda

| TiÃªu chÃ­ | ECS (Fargate) | EKS (Karpenter) | Lambda |
|----------|--------------|-----------------|--------|
| **Scaling speed** | 1-3 phÃºt | 30s-2 phÃºt (Karpenter) | Milliseconds (cÃ³ sáºµn) |
| **Min instances** | 0 (Scale to Zero vá»›i Scheduled) | 0 (KEDA) | 0 (tá»± Ä‘á»™ng) |
| **Max instances** | Fargate quota (per region) | Node limits + vCPU quota | 1000 concurrent (default) |
| **Scaling metric** | CPU, Memory, ALB, Custom | CPU, Memory, Custom (Prometheus) | Tá»± Ä‘á»™ng theo request |
| **Cost khi idle** | Min task count Ã— giÃ¡ | Min node Ã— giÃ¡ | $0 (pay per invocation) |
| **Cold start** | Task startup (30-60s) | Pod + Node startup (30-120s) | 100ms-10s (tÃ¹y runtime) |
| **Best for** | Long-running services | Complex workloads, GPU | Event-driven, bursty |

> ğŸ’¡ **Khi nÃ o chá»n gÃ¬?**
> - **Lambda**: Bursty traffic, event-driven, execution time < 15 phÃºt
> - **ECS Fargate**: Stateless web services, API backends, predictable workloads
> - **EKS**: Complex microservices, cáº§n full Kubernetes ecosystem, GPU workloads

---

## 3. Multi-AZ & Multi-Region

### 3.1. Multi-AZ â€” High Availability trong Region

Multi-AZ lÃ  **má»©c tá»‘i thiá»ƒu** cho production â€” deploy service trÃªn **Ã­t nháº¥t 2 Availability Zones** trong cÃ¹ng Region.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Region: ap-southeast-1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Application Load Balancer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Cross-Zone Load Balancing: ENABLED                                 â”‚  â”‚
â”‚  â”‚  Routes traffic to healthy targets across all AZs                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                       â”‚                       â”‚               â”‚
â”‚           â–¼                       â–¼                       â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€ AZ-1a â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€ AZ-1b â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€ AZ-1c â”€â”€â”  â”‚
â”‚  â”‚                      â”‚  â”‚                      â”‚  â”‚                 â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚ ECS Task Ã—3    â”‚  â”‚  â”‚  â”‚ ECS Task Ã—3    â”‚  â”‚  â”‚  â”‚ ECS TaskÃ—2 â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ Order Service  â”‚  â”‚  â”‚  â”‚ Order Service  â”‚  â”‚  â”‚  â”‚ Order Svc  â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                      â”‚  â”‚                      â”‚  â”‚                 â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚ Aurora         â”‚  â”‚  â”‚  â”‚ Aurora         â”‚  â”‚  â”‚  â”‚ Aurora     â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ Primary        â”‚â”€â”€â”¼â”€â”€â”¼â”€â”€â–¶ Read Replica   â”‚â”€â”€â”¼â”€â”€â”¼â”€â”€â–¶ Read       â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ (Writer)       â”‚  â”‚  â”‚  â”‚ (Reader)       â”‚  â”‚  â”‚  â”‚ Replica    â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                      â”‚  â”‚                      â”‚  â”‚                 â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚ ElastiCache    â”‚  â”‚  â”‚  â”‚ ElastiCache    â”‚  â”‚  â”‚  â”‚ ElastiCacheâ”‚ â”‚  â”‚
â”‚  â”‚  â”‚ Primary        â”‚â”€â”€â”¼â”€â”€â”¼â”€â”€â–¶ Replica        â”‚â”€â”€â”¼â”€â”€â”¼â”€â”€â–¶ Replica    â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                      â”‚  â”‚                      â”‚  â”‚                 â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚ NAT Gateway    â”‚  â”‚  â”‚  â”‚ NAT Gateway    â”‚  â”‚  â”‚  â”‚ NAT GW     â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Multi-AZ Checklist

| Layer | Service | Multi-AZ máº·c Ä‘á»‹nh? | Cáº§n cáº¥u hÃ¬nh? |
|-------|---------|:------------------:|---------------|
| **Compute** | ECS Fargate | âœ… (spread across AZs) | Chá»‰ cáº§n chá»n â‰¥ 2 subnets |
| **Compute** | EKS | âœ… (náº¿u node group multi-AZ) | Karpenter: cáº¥u hÃ¬nh zone topology |
| **Compute** | Lambda | âœ… (tá»± Ä‘á»™ng) | KhÃ´ng cáº§n |
| **Database** | Aurora | âš ï¸ Cáº§n báº­t Multi-AZ | `availability_zones` + replica |
| **Database** | DynamoDB | âœ… (tá»± Ä‘á»™ng replicate 3 AZs) | KhÃ´ng cáº§n |
| **Cache** | ElastiCache | âš ï¸ Cáº§n báº­t Multi-AZ | `automatic_failover_enabled = true` |
| **Queue** | SQS | âœ… (tá»± Ä‘á»™ng) | KhÃ´ng cáº§n |
| **Storage** | S3 | âœ… (tá»± Ä‘á»™ng replicate) | KhÃ´ng cáº§n |
| **Network** | ALB | âœ… (khi chá»n â‰¥ 2 subnets) | Chá»n subnets á»Ÿ nhiá»u AZs |
| **Network** | NAT Gateway | âŒ Single-AZ per gateway | Táº¡o 1 NAT GW má»—i AZ |

### 3.2. Multi-Region â€” Disaster Recovery & Global Users

Multi-Region cáº§n thiáº¿t khi: **(1)** YÃªu cáº§u DR cho critical workloads, **(2)** Giáº£m latency cho global users, **(3)** Compliance â€” data pháº£i á»Ÿ region cá»¥ thá»ƒ.

```
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚      Route 53        â”‚
                         â”‚  Failover Routing    â”‚
                         â”‚  (Health Check based)â”‚
                         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                               â”‚          â”‚
                    Primary    â”‚          â”‚   Secondary
                               â–¼          â–¼
â”Œâ”€â”€â”€â”€ Region: ap-southeast-1 (Primary) â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€ Region: ap-northeast-1 (DR) â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                             â”‚  â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€ ECS / EKS Cluster â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€ ECS / EKS Cluster â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”      â”‚  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”                 â”‚  â”‚
â”‚  â”‚  â”‚Svc Aâ”‚ â”‚Svc Bâ”‚ â”‚Svc Câ”‚ â”‚Svc Dâ”‚      â”‚  â”‚  â”‚  â”‚  â”‚Svc Aâ”‚ â”‚Svc Bâ”‚  (standby)      â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜      â”‚  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                             â”‚  â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Global Database     â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Aurora Primary   â”‚ â”€â”€â”€â”€ async repl â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â–¶ Aurora Replica   â”‚                  â”‚
â”‚  â”‚ (read-write)     â”‚   (~1s lag)           â”‚  â”‚  â”‚ (read-only)      â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                             â”‚  â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Global Tables       â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ DynamoDB Table   â”‚ â”€â”€â”€â”€ active-active â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â–¶ DynamoDB Replica â”‚                  â”‚
â”‚  â”‚ (read-write)     â”‚                       â”‚  â”‚  â”‚ (read-write)     â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                             â”‚  â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Cross-Region Repl   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ S3 Bucket        â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â–¶ S3 Replica       â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                             â”‚  â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3. DR Strategies trÃªn AWS

| Strategy | RTO | RPO | Cost | Khi nÃ o dÃ¹ng |
|----------|-----|-----|------|-------------|
| **Backup & Restore** | Giá» | Giá» | $ | Non-critical, batch workloads |
| **Pilot Light** | 10-30 phÃºt | PhÃºt | $$ | Core services, database replicated |
| **Warm Standby** | PhÃºt | GiÃ¢y-phÃºt | $$$ | Business-critical, cáº§n failover nhanh |
| **Active-Active** | ~0 (near zero) | ~0 | $$$$ | Mission-critical, global users |

> **RTO** (Recovery Time Objective) = thá»i gian tá»‘i Ä‘a cháº¥p nháº­n Ä‘Æ°á»£c Ä‘á»ƒ khÃ´i phá»¥c há»‡ thá»‘ng.
> **RPO** (Recovery Point Objective) = lÆ°á»£ng data tá»‘i Ä‘a cháº¥p nháº­n máº¥t khi sá»± cá»‘ xáº£y ra.

---

## 4. Circuit Breaker & Resilience Patterns trÃªn AWS

### 4.1. Circuit Breaker vá»›i App Mesh (Envoy)

**AWS App Mesh** sá»­ dá»¥ng Envoy Proxy lÃ m sidecar, há»— trá»£ **outlier detection** (tÆ°Æ¡ng Ä‘Æ°Æ¡ng Circuit Breaker) á»Ÿ táº§ng infrastructure â€” khÃ´ng cáº§n code trong application.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          CIRCUIT BREAKER Vá»šI APP MESH                    â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Order Service     â”‚      â”‚   Payment Service   â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚  Application  â”‚  â”‚      â”‚  â”‚  Application  â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â”‚          â”‚          â”‚      â”‚         â–²           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ Envoy Proxy   â”‚â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–¶ Envoy Proxy   â”‚  â”‚    â”‚
â”‚  â”‚  â”‚               â”‚  â”‚      â”‚  â”‚               â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ Outlier Det â”‚  â”‚      â”‚  â”‚ â€¢ Health Checkâ”‚  â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ Retry       â”‚  â”‚      â”‚  â”‚ â€¢ Rate Limit  â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ Timeout     â”‚  â”‚      â”‚  â”‚               â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ Circuit Brk â”‚  â”‚      â”‚  â”‚               â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                          â”‚
â”‚  App Mesh Control Plane â† cáº¥u hÃ¬nh qua API/Terraform     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Cáº¥u hÃ¬nh Outlier Detection (Circuit Breaker)

```hcl
# App Mesh Virtual Node vá»›i Outlier Detection
resource "aws_appmesh_virtual_node" "payment_service" {
  name      = "payment-service"
  mesh_name = aws_appmesh_mesh.main.name

  spec {
    listener {
      port_mapping {
        port     = 8080
        protocol = "http"
      }

      # Connection pool â€” Bulkhead pattern
      connection_pool {
        http {
          max_connections      = 100   # Max concurrent connections
          max_pending_requests = 50    # Max queued requests
        }
      }

      # Outlier Detection â€” Circuit Breaker
      outlier_detection {
        max_server_errors    = 5              # 5 lá»—i 5xx liÃªn tiáº¿p
        interval {
          value = 30
          unit  = "s"                         # Check má»—i 30 giÃ¢y
        }
        base_ejection_duration {
          value = 30
          unit  = "s"                         # Eject (open circuit) 30 giÃ¢y
        }
        max_ejection_percent = 50             # Tá»‘i Ä‘a eject 50% endpoints
      }

      # Timeout
      timeout {
        http {
          per_request {
            value = 15
            unit  = "s"                       # Timeout má»—i request: 15s
          }
          idle {
            value = 300
            unit  = "s"                       # Idle timeout: 5 phÃºt
          }
        }
      }
    }

    # Retry Policy
    listener {
      port_mapping {
        port     = 8080
        protocol = "http"
      }
    }

    service_discovery {
      aws_cloud_map {
        namespace_name = aws_service_discovery_private_dns_namespace.main.name
        service_name   = "payment-service"
      }
    }
  }
}

# Virtual Router vá»›i Retry Policy
resource "aws_appmesh_route" "payment_route" {
  name                = "payment-route"
  mesh_name           = aws_appmesh_mesh.main.name
  virtual_router_name = aws_appmesh_virtual_router.payment.name

  spec {
    http_route {
      match {
        prefix = "/"
      }

      action {
        weighted_target {
          virtual_node = aws_appmesh_virtual_node.payment_service.name
          weight       = 100
        }
      }

      # Retry Policy
      retry_policy {
        max_retries = 3

        http_retry_events = [
          "server-error",      # 5xx
          "gateway-error",     # 502, 503, 504
        ]

        tcp_retry_events = [
          "connection-error",
        ]

        per_retry_timeout {
          value = 5
          unit  = "s"
        }
      }

      # Timeout
      timeout {
        per_request {
          value = 30
          unit  = "s"
        }
        idle {
          value = 60
          unit  = "s"
        }
      }
    }
  }
}
```

#### App Mesh vs Application-Level Circuit Breaker

| TiÃªu chÃ­ | App Mesh (Envoy) | Application-Level (Resilience4j, Polly) |
|----------|------------------|----------------------------------------|
| **NgÃ´n ngá»¯** | Language-agnostic | Per-language library |
| **Config** | Infrastructure-as-Code | Application config |
| **Granularity** | Per-service endpoint | Per-method/per-call |
| **Custom logic** | âŒ Limited | âœ… Full control (fallback, custom metrics) |
| **Overhead** | Sidecar resource (~50MB RAM) | In-process (minimal) |
| **Khi nÃ o dÃ¹ng** | Polyglot services, platform-level policy | Fine-grained control, complex fallbacks |

> ğŸ’¡ **Best practice**: DÃ¹ng **App Mesh cho baseline** (timeout, retry, outlier detection), káº¿t há»£p **application-level cho business logic** (custom fallback, degraded response).

### 4.2. Retry & Timeout trÃªn AWS

NgoÃ i App Mesh, nhiá»u AWS service cÃ³ **built-in retry**:

| Service | Retry tÃ­ch há»£p | Cáº¥u hÃ¬nh |
|---------|:--------------:|----------|
| **API Gateway** | âŒ | Cáº§n tá»± handle á»Ÿ backend |
| **ALB** | âŒ | Retry á»Ÿ client hoáº·c service mesh |
| **SQS** | âœ… | `maxReceiveCount` â†’ DLQ |
| **Step Functions** | âœ… | `Retry` field trong state definition |
| **EventBridge** | âœ… | Retry policy (max 185 láº§n trong 24h) |
| **Lambda (async)** | âœ… | 2 retries máº·c Ä‘á»‹nh â†’ DLQ/event destination |
| **SNS** | âœ… | Retry policy per subscription |

#### Step Functions â€” Retry & Error Handling

```json
{
  "ProcessPayment": {
    "Type": "Task",
    "Resource": "arn:aws:lambda:ap-southeast-1:123456789:function:process-payment",
    "TimeoutSeconds": 30,
    "HeartbeatSeconds": 10,
    "Retry": [
      {
        "ErrorEquals": ["PaymentGatewayTimeout", "Lambda.ServiceException"],
        "IntervalSeconds": 2,
        "MaxAttempts": 3,
        "BackoffRate": 2.0
      },
      {
        "ErrorEquals": ["States.TaskFailed"],
        "IntervalSeconds": 5,
        "MaxAttempts": 2,
        "BackoffRate": 1.5
      }
    ],
    "Catch": [
      {
        "ErrorEquals": ["PaymentDeclined"],
        "Next": "NotifyPaymentFailed",
        "ResultPath": "$.error"
      },
      {
        "ErrorEquals": ["States.ALL"],
        "Next": "CompensateOrder",
        "ResultPath": "$.error"
      }
    ],
    "Next": "ConfirmOrder"
  }
}
```

### 4.3. Bulkhead trÃªn AWS

Bulkhead trÃªn AWS Ä‘Æ°á»£c triá»ƒn khai qua nhiá»u táº§ng:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  BULKHEAD trÃªn AWS                         â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€ Infrastructure-Level Bulkhead â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  â€¢ Separate ECS Services (khÃ´ng chia sáº» task)      â”‚    â”‚
â”‚  â”‚  â€¢ Separate EKS namespaces + resource quotas       â”‚    â”‚
â”‚  â”‚  â€¢ Lambda reserved concurrency per function        â”‚    â”‚
â”‚  â”‚  â€¢ Separate VPCs/subnets cho critical services     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€ Service-Level Bulkhead â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  â€¢ App Mesh connection pool (max_connections)      â”‚    â”‚
â”‚  â”‚  â€¢ ALB target group per service                    â”‚    â”‚
â”‚  â”‚  â€¢ API Gateway usage plans per client              â”‚    â”‚
â”‚  â”‚  â€¢ SQS separate queues per workload type           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€ Database-Level Bulkhead â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  â€¢ Database per Service (doc 09, doc 20)           â”‚    â”‚
â”‚  â”‚  â€¢ Aurora connection limits per service            â”‚    â”‚
â”‚  â”‚  â€¢ DynamoDB separate tables per service            â”‚    â”‚
â”‚  â”‚  â€¢ ElastiCache separate clusters for critical data â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.4. Rate Limiting trÃªn AWS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RATE LIMITING LAYERS trÃªn AWS              â”‚
â”‚                                                         â”‚
â”‚  Layer 1: CloudFront                                    â”‚
â”‚  â”œâ”€â”€ AWS WAF Rate-based Rules                           â”‚
â”‚  â”œâ”€â”€ Giá»›i háº¡n per IP: vÃ­ dá»¥ 2000 req / 5 min            â”‚
â”‚  â””â”€â”€ Block DDoS táº¡i edge (trÆ°á»›c khi vÃ o Region)         â”‚
â”‚                                                         â”‚
â”‚  Layer 2: API Gateway                                   â”‚
â”‚  â”œâ”€â”€ Usage Plans + API Keys                             â”‚
â”‚  â”œâ”€â”€ Per-client throttling: 100 req/s burst, 50 req/s   â”‚
â”‚  â”œâ”€â”€ Account-level: 10,000 req/s (default)              â”‚
â”‚  â””â”€â”€ Method-level throttling (override per endpoint)    â”‚
â”‚                                                         â”‚
â”‚  Layer 3: ALB + WAF                                     â”‚
â”‚  â”œâ”€â”€ WAF Rate-based Rules trÃªn ALB                      â”‚
â”‚  â””â”€â”€ Giá»›i háº¡n per IP hoáº·c per header                    â”‚
â”‚                                                         â”‚
â”‚  Layer 4: Application                                   â”‚
â”‚  â”œâ”€â”€ Redis-based rate limiter (ElastiCache)             â”‚
â”‚  â”œâ”€â”€ Token Bucket / Sliding Window algorithm            â”‚
â”‚  â””â”€â”€ Per-user, per-tenant, per-API-key                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### API Gateway Throttling (Terraform)

```hcl
# Usage Plan â€” Rate limiting per client
resource "aws_api_gateway_usage_plan" "partner_plan" {
  name = "partner-tier"

  throttle_settings {
    burst_limit = 200     # Max burst requests
    rate_limit  = 100     # Sustained requests per second
  }

  quota_settings {
    limit  = 100000       # 100K requests per month
    period = "MONTH"
  }

  api_stages {
    api_id = aws_api_gateway_rest_api.main.id
    stage  = aws_api_gateway_stage.prod.stage_name

    # Override cho specific endpoint
    throttle {
      path        = "/orders/POST"
      burst_limit = 50
      rate_limit  = 20
    }
  }
}
```

### 4.5. Fallback Patterns trÃªn AWS

| Pattern | AWS Implementation | VÃ­ dá»¥ |
|---------|-------------------|-------|
| **Cache Fallback** | ElastiCache (Redis) | Product catalog: DB down â†’ serve from cache |
| **Queue Fallback** | SQS | Payment timeout â†’ queue for retry later |
| **Static Fallback** | S3 + CloudFront | Service down â†’ serve static response |
| **Default Response** | API Gateway Mock | Recommendation down â†’ return default list |
| **Async Fallback** | EventBridge + Lambda | Sync call fails â†’ emit event for async processing |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        FALLBACK FLOW â€” Product Service                 â”‚
â”‚                                                        â”‚
â”‚  Request â”€â”€â–¶ Product Service                           â”‚
â”‚                 â”‚                                      â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                                â”‚
â”‚           â”‚ DB Query  â”‚                                â”‚
â”‚           â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                 â”‚                                      â”‚
â”‚          Successâ”‚     Fail/Timeout                     â”‚
â”‚                 â”‚         â”‚                            â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚           â”‚Responseâ”‚  â”‚ Redis Cache â”‚                  â”‚
â”‚           â”‚ (fresh)â”‚  â”‚ (stale OK)  â”‚                  â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                          â”‚                             â”‚
â”‚                   Cache Hitâ”‚    Cache Miss             â”‚
â”‚                          â”‚         â”‚                   â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚                    â”‚Responseâ”‚  â”‚ S3 Static   â”‚         â”‚
â”‚                    â”‚(cached)â”‚  â”‚ (default)   â”‚         â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Health Check & Self-Healing

### 5.1. Health Check trÃªn ECS

ECS há»— trá»£ **2 loáº¡i health check**: Container-level vÃ  ALB Target Group.

```hcl
# ECS Task Definition â€” Container Health Check
resource "aws_ecs_task_definition" "order_service" {
  family = "order-service"

  container_definitions = jsonencode([{
    name  = "order-service"
    image = "123456789.dkr.ecr.ap-southeast-1.amazonaws.com/order-service:latest"

    portMappings = [{
      containerPort = 8080
      protocol      = "tcp"
    }]

    # Container Health Check
    healthCheck = {
      command     = ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval    = 30        # Check má»—i 30 giÃ¢y
      timeout     = 5         # Timeout per check: 5 giÃ¢y
      retries     = 3         # 3 láº§n fail â†’ unhealthy
      startPeriod = 60        # Grace period sau khi start: 60 giÃ¢y
    }

    # Resource limits
    cpu    = 512
    memory = 1024
  }])
}
```

#### Health Check Endpoint Design

```
GET /health          â†’ Basic liveness (luÃ´n return 200 náº¿u process sá»‘ng)
GET /health/ready    â†’ Readiness (check dependencies: DB, cache, queue)
GET /health/detailed â†’ Deep check (chi tiáº¿t tá»«ng dependency â€” chá»‰ internal)
```

```json
// GET /health/ready â€” 200 OK
{
  "status": "UP",
  "checks": {
    "database": { "status": "UP", "responseTime": "12ms" },
    "redis": { "status": "UP", "responseTime": "2ms" },
    "sqs": { "status": "UP" }
  }
}

// GET /health/ready â€” 503 Service Unavailable
{
  "status": "DOWN",
  "checks": {
    "database": { "status": "DOWN", "error": "Connection timeout" },
    "redis": { "status": "UP", "responseTime": "2ms" },
    "sqs": { "status": "UP" }
  }
}
```

### 5.2. Health Check trÃªn EKS

```yaml
# Kubernetes Pod â€” Liveness & Readiness Probes
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service
spec:
  template:
    spec:
      containers:
        - name: order-service
          image: order-service:latest
          ports:
            - containerPort: 8080

          # Liveness Probe â€” restart container náº¿u fail
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30     # Chá» 30s sau khi start
            periodSeconds: 10           # Check má»—i 10s
            timeoutSeconds: 3
            failureThreshold: 3         # 3 láº§n fail â†’ restart

          # Readiness Probe â€” remove khá»i Service endpoints náº¿u fail
          readinessProbe:
            httpGet:
              path: /health/ready
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 2         # 2 láº§n fail â†’ stop traffic

          # Startup Probe â€” cho phÃ©p app khá»Ÿi Ä‘á»™ng cháº­m
          startupProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 5
            failureThreshold: 30        # 30 Ã— 5s = 150s max startup time
```

### 5.3. ALB/NLB Health Check

```hcl
# ALB Target Group Health Check
resource "aws_lb_target_group" "order_service" {
  name        = "order-service-tg"
  port        = 8080
  protocol    = "HTTP"
  target_type = "ip"
  vpc_id      = aws_vpc.main.id

  health_check {
    enabled             = true
    path                = "/health/ready"
    port                = "traffic-port"
    protocol            = "HTTP"
    healthy_threshold   = 2          # 2 láº§n OK â†’ healthy
    unhealthy_threshold = 3          # 3 láº§n fail â†’ unhealthy â†’ stop traffic
    timeout             = 5
    interval            = 15         # Check má»—i 15 giÃ¢y
    matcher             = "200"      # Chá»‰ accept 200
  }

  # Deregistration delay â€” cho phÃ©p drain connections
  deregistration_delay = 30

  stickiness {
    type            = "lb_cookie"
    cookie_duration = 86400
    enabled         = false          # Stateless service â†’ khÃ´ng cáº§n sticky
  }
}
```

### 5.4. Route 53 Health Check

Route 53 Health Check dÃ¹ng cho **DNS-level failover** â€” chuyá»ƒn traffic giá»¯a regions hoáº·c endpoints.

```hcl
# Route 53 Health Check â€” Primary Region
resource "aws_route53_health_check" "primary" {
  fqdn              = "api-primary.example.com"
  port               = 443
  type               = "HTTPS"
  resource_path      = "/health"
  failure_threshold  = 3
  request_interval   = 10         # Check má»—i 10 giÃ¢y (fast)

  regions = [
    "us-east-1",
    "eu-west-1",
    "ap-southeast-1"              # Check tá»« 3 regions
  ]

  tags = {
    Name = "primary-region-health"
  }
}

# CloudWatch Alarm cho Health Check
resource "aws_route53_health_check" "primary_alarm" {
  type                = "CLOUDWATCH_METRIC"
  cloudwatch_alarm_name   = aws_cloudwatch_metric_alarm.primary_error_rate.alarm_name
  cloudwatch_alarm_region = "ap-southeast-1"
  insufficient_data_health_status = "LastKnownStatus"
}

# DNS Failover Record
resource "aws_route53_record" "api" {
  zone_id = aws_route53_zone.main.zone_id
  name    = "api.example.com"
  type    = "A"

  # Primary
  failover_routing_policy {
    type = "PRIMARY"
  }

  alias {
    name                   = aws_lb.primary.dns_name
    zone_id                = aws_lb.primary.zone_id
    evaluate_target_health = true
  }

  set_identifier  = "primary"
  health_check_id = aws_route53_health_check.primary.id
}

resource "aws_route53_record" "api_dr" {
  zone_id = aws_route53_zone.main.zone_id
  name    = "api.example.com"
  type    = "A"

  # Secondary (DR)
  failover_routing_policy {
    type = "SECONDARY"
  }

  alias {
    name                   = aws_lb.dr.dns_name
    zone_id                = aws_lb.dr.zone_id
    evaluate_target_health = true
  }

  set_identifier = "secondary"
}
```

---

## 6. Chaos Engineering trÃªn AWS

### 6.1. AWS Fault Injection Service (FIS)

**AWS FIS** (Fault Injection Service) lÃ  managed service cho Chaos Engineering â€” inject failures vÃ o há»‡ thá»‘ng Ä‘á»ƒ kiá»ƒm tra resilience.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AWS FAULT INJECTION SERVICE                   â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€ Experiment Template â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Define:                                           â”‚    â”‚
â”‚  â”‚  â€¢ Targets: EC2, ECS, EKS Pods, RDS, etc.          â”‚    â”‚
â”‚  â”‚  â€¢ Actions: stop instance, kill container, etc.    â”‚    â”‚
â”‚  â”‚  â€¢ Stop conditions: CloudWatch alarm thresholds    â”‚    â”‚
â”‚  â”‚  â€¢ Duration: how long to inject fault              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                                                â”‚
â”‚           â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€ Run Experiment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  1. FIS inject fault vÃ o target                    â”‚    â”‚
â”‚  â”‚  2. Monitor via CloudWatch                         â”‚    â”‚
â”‚  â”‚  3. Stop condition triggered â†’ auto rollback       â”‚    â”‚
â”‚  â”‚  4. Analyze results â†’ improve resilience           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### FIS Experiment Template (Terraform)

```hcl
# FIS Experiment â€” Kill ECS Tasks
resource "aws_fis_experiment_template" "ecs_task_kill" {
  description = "Kill 50% ECS tasks to test auto-recovery"
  role_arn    = aws_iam_role.fis_role.arn

  # Stop condition â€” dá»«ng náº¿u error rate quÃ¡ cao
  stop_condition {
    source = "aws:cloudwatch:alarm"
    value  = aws_cloudwatch_metric_alarm.high_error_rate.arn
  }

  # Target: ECS Tasks
  target {
    name           = "ecs-tasks"
    resource_type  = "aws:ecs:task"
    selection_mode = "PERCENT(50)"       # Kill 50% tasks

    resource_tag {
      key   = "Environment"
      value = "staging"                  # Chá»‰ staging!
    }

    filter {
      path   = "State.Name"
      values = ["RUNNING"]
    }
  }

  # Action: Stop ECS Tasks
  action {
    name        = "stop-ecs-tasks"
    action_id   = "aws:ecs:stop-task"
    description = "Stop 50% of order-service tasks"

    target {
      key   = "Tasks"
      value = "ecs-tasks"
    }
  }

  tags = {
    Experiment = "ecs-resilience-test"
    Team       = "platform"
  }
}

# FIS Experiment â€” Network Latency
resource "aws_fis_experiment_template" "network_latency" {
  description = "Add 500ms latency to simulate slow network"
  role_arn    = aws_iam_role.fis_role.arn

  stop_condition {
    source = "aws:cloudwatch:alarm"
    value  = aws_cloudwatch_metric_alarm.p99_latency.arn
  }

  target {
    name           = "ec2-instances"
    resource_type  = "aws:ec2:instance"
    selection_mode = "ALL"

    resource_tag {
      key   = "Service"
      value = "payment-service"
    }
  }

  action {
    name        = "inject-latency"
    action_id   = "aws:ssm:send-command"
    description = "Inject 500ms network latency"

    parameter {
      key   = "documentArn"
      value = "arn:aws:ssm:ap-southeast-1::document/AWSFIS-Run-Network-Latency"
    }
    parameter {
      key   = "documentParameters"
      value = jsonencode({
        DurationSeconds  = "300"
        DelayMilliseconds = "500"
        Interface         = "eth0"
      })
    }
    parameter {
      key   = "duration"
      value = "PT5M"                    # 5 phÃºt
    }

    target {
      key   = "Instances"
      value = "ec2-instances"
    }
  }
}
```

### 6.2. CÃ¡c thÃ­ nghiá»‡m Chaos phá»• biáº¿n

| ThÃ­ nghiá»‡m | Target | Ká»³ vá»ng | FIS Action |
|------------|--------|---------|------------|
| **Kill tasks/pods** | ECS Tasks / EKS Pods | Auto Scaling táº¡o tasks má»›i, zero downtime | `aws:ecs:stop-task` / `aws:eks:terminate-nodegroup-instances` |
| **AZ failure** | Subnet / AZ | Traffic chuyá»ƒn sang AZ khÃ¡c, ALB healthy | `aws:ec2:stop-instances` (all in 1 AZ) |
| **Network latency** | EC2 / ECS | Circuit breaker kÃ­ch hoáº¡t, fallback hoáº¡t Ä‘á»™ng | `AWSFIS-Run-Network-Latency` |
| **CPU stress** | EC2 / ECS | Auto Scaling tÄƒng instances, latency khÃ´ng tÄƒng quÃ¡ SLA | `AWSFIS-Run-CPU-Stress` |
| **DB failover** | RDS / Aurora | Automatic failover, app reconnect trong vÃ i giÃ¢y | `aws:rds:failover-db-cluster` |
| **DNS failure** | Route 53 | Failover sang secondary region | `aws:route53:update-healthcheck` |

### 6.3. Game Day â€” Quy trÃ¬nh thá»±c hÃ nh

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GAME DAY PROCESS                        â”‚
â”‚                                                            â”‚
â”‚  Phase 1: CHUáº¨N Bá»Š                                         â”‚
â”‚  â”œâ”€â”€ 1. XÃ¡c Ä‘á»‹nh scope (service nÃ o, environment nÃ o)      â”‚
â”‚  â”œâ”€â”€ 2. Äáº·t giáº£ thuyáº¿t ("Náº¿u AZ-a down, há»‡ thá»‘ng váº«n       â”‚
â”‚  â”‚       serve 100% traffic vá»›i p99 < 500ms")              â”‚
â”‚  â”œâ”€â”€ 3. Thiáº¿t láº­p monitoring dashboard                     â”‚
â”‚  â”œâ”€â”€ 4. Define stop conditions (abort criteria)            â”‚
â”‚  â””â”€â”€ 5. Notify stakeholders                                â”‚
â”‚                                                            â”‚
â”‚  Phase 2: THá»°C HIá»†N                                        â”‚
â”‚  â”œâ”€â”€ 1. Start monitoring (record baseline metrics)         â”‚
â”‚  â”œâ”€â”€ 2. Inject fault (qua FIS hoáº·c manual)                 â”‚
â”‚  â”œâ”€â”€ 3. Observe â€” so sÃ¡nh metrics vs giáº£ thuyáº¿t            â”‚
â”‚  â”œâ”€â”€ 4. Escalate náº¿u cáº§n (trigger stop condition)          â”‚
â”‚  â””â”€â”€ 5. Rollback fault injection                           â”‚
â”‚                                                            â”‚
â”‚  Phase 3: PHÃ‚N TÃCH                                        â”‚
â”‚  â”œâ”€â”€ 1. So sÃ¡nh káº¿t quáº£ vs giáº£ thuyáº¿t                      â”‚
â”‚  â”œâ”€â”€ 2. Document findings (gÃ¬ hoáº¡t Ä‘á»™ng, gÃ¬ khÃ´ng)         â”‚
â”‚  â”œâ”€â”€ 3. Táº¡o action items (fix gaps)                        â”‚
â”‚  â”œâ”€â”€ 4. Update runbooks náº¿u cáº§n                            â”‚
â”‚  â””â”€â”€ 5. Schedule next Game Day                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> ğŸ’¡ **Quy táº¯c vÃ ng**: **Báº¯t Ä‘áº§u tá»« staging**, chá»‰ cháº¡y production khi Ä‘Ã£ tá»± tin. LuÃ´n cÃ³ **stop condition** rÃµ rÃ ng. **Blast radius nhá»** â€” tÄƒng dáº§n scope qua tá»«ng Game Day.

---

## 7. Disaster Recovery trÃªn AWS

### 7.1. Bá»‘n chiáº¿n lÆ°á»£c DR

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DR STRATEGIES SPECTRUM                         â”‚
â”‚                                                                 â”‚
â”‚  Cost tháº¥p â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Cost cao    â”‚
â”‚  RTO dÃ i   â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º RTO ngáº¯n    â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Backup &   â”‚  â”‚   Pilot     â”‚  â”‚  Warm    â”‚  â”‚  Active-  â”‚  â”‚
â”‚  â”‚  Restore    â”‚  â”‚   Light     â”‚  â”‚  Standby â”‚  â”‚  Active   â”‚  â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚          â”‚  â”‚           â”‚  â”‚
â”‚  â”‚ RTO: giá»    â”‚  â”‚ RTO: 10-30m â”‚  â”‚ RTO: phÃºtâ”‚  â”‚ RTO: ~0   â”‚  â”‚
â”‚  â”‚ RPO: giá»    â”‚  â”‚ RPO: phÃºt   â”‚  â”‚ RPO: giÃ¢yâ”‚  â”‚ RPO: ~0   â”‚  â”‚
â”‚  â”‚ Cost: $     â”‚  â”‚ Cost: $$    â”‚  â”‚ Cost: $$$â”‚  â”‚ Cost: $$$$â”‚  â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚          â”‚  â”‚           â”‚  â”‚
â”‚  â”‚ S3 backups  â”‚  â”‚ DB replica  â”‚  â”‚ Scaled   â”‚  â”‚ Full      â”‚  â”‚
â”‚  â”‚ AMI copies  â”‚  â”‚ Min compute â”‚  â”‚ down     â”‚  â”‚ capacity  â”‚  â”‚
â”‚  â”‚ No compute  â”‚  â”‚ Core only   â”‚  â”‚ compute  â”‚  â”‚ both      â”‚  â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚          â”‚  â”‚ regions   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2. DR cho Microservice â€” Thiáº¿t káº¿ chi tiáº¿t

#### Pilot Light â€” Phá»• biáº¿n nháº¥t cho Microservice

```
â”Œâ”€â”€â”€â”€ Primary Region â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€ DR Region â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                      â”‚   â”‚                                    â”‚
â”‚  ECS Cluster (ACTIVE)                â”‚   â”‚  ECS Cluster (STOPPED)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”          â”‚   â”‚  Task definitions ready            â”‚
â”‚  â”‚ Ã—5   â”‚ â”‚ Ã—3   â”‚ â”‚ Ã—3   â”‚          â”‚   â”‚  ECR images replicated             â”‚
â”‚  â”‚Order â”‚ â”‚Pay   â”‚ â”‚Notif â”‚          â”‚   â”‚  desired_count = 0                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜          â”‚   â”‚                                    â”‚
â”‚                                      â”‚   â”‚                                    â”‚
â”‚  Aurora Primary â”€â”€â”€ Global DB â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â–¶  Aurora Replica (ALWAYS ON)        â”‚
â”‚  (read-write)    (async, ~1s lag)    â”‚   â”‚  (read-only, promote khi failover) â”‚
â”‚                                      â”‚   â”‚                                    â”‚
â”‚  DynamoDB â”€â”€â”€ Global Tables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â–¶  DynamoDB Replica (ALWAYS ON)      â”‚
â”‚  (read-write)  (active-active)       â”‚   â”‚  (read-write ready)                â”‚
â”‚                                      â”‚   â”‚                                    â”‚
â”‚  ElastiCache â”€â”€â”€ âŒ No replication â”€â”€â”¼â”€â”€â”€â”‚  ElastiCache (cold, warm on DR)    â”‚
â”‚                                      â”‚   â”‚                                    â”‚
â”‚  S3 â”€â”€â”€ Cross-Region Replication â”€â”€â”€â”€â”¼â”€â”€â”€â–¶  S3 Replica                        â”‚
â”‚                                      â”‚   â”‚                                    â”‚
â”‚  Secrets Manager (replicated) â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â–¶  Secrets (ALWAYS AVAILABLE)        â”‚
â”‚                                      â”‚   â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Failover Steps (automated via Lambda + Step Functions):
1. Route 53 health check fails â†’ trigger failover
2. Step Functions orchestrate:
   a. Promote Aurora Replica â†’ Primary
   b. Update ECS service desired_count (0 â†’ N)
   c. Create ElastiCache cluster (from snapshot hoáº·c empty)
   d. Update Route 53 DNS â†’ DR region ALB
3. Total time: 10-30 phÃºt
```

#### Active-Active â€” Cho Mission-Critical Services

```hcl
# DynamoDB Global Tables â€” Active-Active
resource "aws_dynamodb_table" "orders" {
  name         = "orders"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "orderId"

  attribute {
    name = "orderId"
    type = "S"
  }

  # Enable Global Tables
  replica {
    region_name = "ap-northeast-1"     # DR Region
  }

  # Point-in-time recovery
  point_in_time_recovery {
    enabled = true
  }
}

# Aurora Global Database
resource "aws_rds_global_cluster" "main" {
  global_cluster_identifier = "ecommerce-global"
  engine                    = "aurora-postgresql"
  engine_version            = "15.4"
  storage_encrypted         = true
}

resource "aws_rds_cluster" "primary" {
  cluster_identifier        = "ecommerce-primary"
  global_cluster_identifier = aws_rds_global_cluster.main.id
  engine                    = aws_rds_global_cluster.main.engine
  engine_version            = aws_rds_global_cluster.main.engine_version
  availability_zones        = ["ap-southeast-1a", "ap-southeast-1b", "ap-southeast-1c"]
  master_username           = "admin"
  master_password           = var.db_password
}

resource "aws_rds_cluster" "dr" {
  provider                  = aws.dr_region
  cluster_identifier        = "ecommerce-dr"
  global_cluster_identifier = aws_rds_global_cluster.main.id
  engine                    = aws_rds_global_cluster.main.engine
  engine_version            = aws_rds_global_cluster.main.engine_version
  availability_zones        = ["ap-northeast-1a", "ap-northeast-1b", "ap-northeast-1c"]

  # DR cluster khÃ´ng cáº§n master credentials (replicate tá»« primary)
}
```

### 7.3. Failover tá»± Ä‘á»™ng vá»›i Route 53

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            AUTOMATIC FAILOVER FLOW                         â”‚
â”‚                                                            â”‚
â”‚  Route 53 Health Checker (3 regions)                       â”‚
â”‚         â”‚                                                  â”‚
â”‚         â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Healthy     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Check primaryâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Route to Primary â”‚      â”‚
â”‚  â”‚ /health      â”‚                â”‚ Region ALB       â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                                                  â”‚
â”‚         â”‚ 3 consecutive failures                           â”‚
â”‚         â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚Mark UNHEALTHYâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Route to DR      â”‚      â”‚
â”‚  â”‚              â”‚                â”‚ Region ALB       â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                                                  â”‚
â”‚         â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚ CloudWatch   â”‚â”€â”€ SNS â”€â”€â–¶ PagerDuty/Slack notification   â”‚
â”‚  â”‚ Alarm        â”‚                                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚         â”‚                                                  â”‚
â”‚         â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚ EventBridge  â”‚â”€â”€ Step Functions â”€â”€â–¶ Auto promote DB     â”‚
â”‚  â”‚ Rule         â”‚                     â–¶ Scale up DR ECS    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â–¶ Warm up caches     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8. VÃ­ dá»¥ thá»±c táº¿ â€” E-Commerce Resilience Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              E-COMMERCE RESILIENCE ARCHITECTURE trÃªn AWS                 â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€ Edge Layer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  CloudFront + WAF                                                â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ DDoS protection (Shield Standard â€” free)                    â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Rate limiting: 2000 req/5min per IP                         â”‚    â”‚
â”‚  â”‚  â””â”€â”€ Static fallback: S3 maintenance page                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€ API Layer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  API Gateway (Regional)                                          â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Throttling: 1000 req/s per client                           â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Usage Plans per tier (Free/Pro/Enterprise)                  â”‚    â”‚
â”‚  â”‚  â””â”€â”€ Lambda Authorizer (JWT validation)                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€ Service Layer (ECS Fargate + App Mesh) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚    â”‚
â”‚  â”‚  â”‚ Order Svc   â”‚  â”‚ Payment Svc â”‚  â”‚ Inventory    â”‚              â”‚    â”‚
â”‚  â”‚  â”‚ min:3 max:20â”‚  â”‚ min:2 max:15â”‚  â”‚ min:2 max:10 â”‚              â”‚    â”‚
â”‚  â”‚  â”‚             â”‚  â”‚             â”‚  â”‚              â”‚              â”‚    â”‚
â”‚  â”‚  â”‚ Envoy:      â”‚  â”‚ Envoy:      â”‚  â”‚ Envoy:       â”‚              â”‚    â”‚
â”‚  â”‚  â”‚ â€¢timeout:15sâ”‚  â”‚ â€¢timeout:30sâ”‚  â”‚ â€¢timeout:10s â”‚              â”‚    â”‚
â”‚  â”‚  â”‚ â€¢retry: 2   â”‚  â”‚ â€¢retry: 1   â”‚  â”‚ â€¢retry: 3    â”‚              â”‚    â”‚
â”‚  â”‚  â”‚ â€¢CB: 5err/30sâ”‚ â”‚ â€¢CB: 3err/30sâ”‚ â”‚ â€¢CB: 5err/30sâ”‚              â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚    â”‚
â”‚  â”‚         â”‚                â”‚                â”‚                      â”‚    â”‚
â”‚  â”‚  Auto Scaling:           â”‚                â”‚                      â”‚    â”‚
â”‚  â”‚  â€¢ CPU Target: 70%       â”‚                â”‚                      â”‚    â”‚
â”‚  â”‚  â€¢ Scale out: 60s cool   â”‚                â”‚                      â”‚    â”‚
â”‚  â”‚  â€¢ Scale in: 300s cool   â”‚                â”‚                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€ Data Layer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Aurora PostgreSQL (Multi-AZ, 2 read replicas)                   â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Auto failover: ~30 giÃ¢y                                     â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Aurora Auto Scaling read replicas (1-5)                     â”‚    â”‚
â”‚  â”‚  â””â”€â”€ Global Database â†’ DR region                                 â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  DynamoDB (On-Demand, Global Tables)                             â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Auto scaling RCU/WCU (náº¿u Provisioned mode)                 â”‚    â”‚
â”‚  â”‚  â””â”€â”€ Point-in-time recovery enabled                              â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  ElastiCache Redis (Multi-AZ, auto failover)                     â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Cache fallback cho Product catalog                          â”‚    â”‚
â”‚  â”‚  â””â”€â”€ Session store (stateless services)                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€ Async Layer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  SQS Queues (per service) + DLQ                                  â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Order Queue: maxReceiveCount=5 â†’ DLQ                        â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Payment Queue: maxReceiveCount=3 â†’ DLQ                      â”‚    â”‚
â”‚  â”‚  â””â”€â”€ Notification Queue: maxReceiveCount=10 â†’ DLQ                â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  EventBridge                                                     â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Retry policy: 3 retries                                     â”‚    â”‚
â”‚  â”‚  â””â”€â”€ DLQ for failed events                                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€ DR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Strategy: Pilot Light                                           â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Aurora Global Database (always replicating)                 â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ DynamoDB Global Tables (active-active)                      â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ ECS task definitions ready (desired_count = 0)              â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Route 53 failover routing                                   â”‚    â”‚
â”‚  â”‚  â””â”€â”€ Failover automation: Step Functions + Lambda                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 9. Anti-patterns

| Anti-pattern | Váº¥n Ä‘á» | CÃ¡ch kháº¯c phá»¥c |
|-------------|--------|---------------|
| **No health checks** | Container crash nhÆ°ng váº«n nháº­n traffic | LuÃ´n cÃ³ liveness + readiness probes |
| **Same scaling metric cho má»i service** | CPU-based scaling cho I/O-bound service â†’ scale khÃ´ng hiá»‡u quáº£ | Chá»n metric phÃ¹ há»£p workload (SQS depth, RPS, custom) |
| **Scale in quÃ¡ nhanh** | Flapping: scale out â†’ scale in â†’ scale out liÃªn tá»¥c | Scale in cooldown â‰¥ 300s, stabilization window |
| **KhÃ´ng test DR** | DR plan trÃªn giáº¥y, thá»±c táº¿ khÃ´ng hoáº¡t Ä‘á»™ng | Game Day hÃ ng quÃ½, automated DR testing |
| **Retry everything** | Retry non-idempotent operations â†’ duplicate data | Chá»‰ retry idempotent operations, dÃ¹ng idempotency key |
| **Circuit Breaker má»Ÿ quÃ¡ sá»›m** | 1-2 lá»—i â†’ open circuit â†’ service unavailable | Threshold Ä‘á»§ cao (5-10 errors), window Ä‘á»§ dÃ i (30-60s) |
| **Single NAT Gateway** | NAT GW single AZ down â†’ táº¥t cáº£ private subnets máº¥t internet | 1 NAT GW per AZ |
| **No DLQ** | Failed messages máº¥t vÄ©nh viá»…n, khÃ´ng retry Ä‘Æ°á»£c | LuÃ´n cÃ³ DLQ cho SQS, Lambda async, EventBridge |
| **Over-provisioned "just in case"** | Chi phÃ­ cloud phÃ¬nh to khÃ´ng cáº§n thiáº¿t | Right-sizing, auto scaling, Spot instances |

---

## 10. Checklist triá»ƒn khai

### Auto Scaling

- [ ] ECS/EKS services cÃ³ auto scaling policy (Target Tracking)
- [ ] Scale out cooldown â‰¤ 60s, scale in cooldown â‰¥ 300s
- [ ] Min capacity Ä‘á»§ cho baseline traffic
- [ ] Max capacity Ä‘á»§ cho peak traffic (+ buffer 20%)
- [ ] Custom metrics cho business-specific scaling (SQS depth, RPS)
- [ ] Lambda reserved concurrency cho critical functions
- [ ] Provisioned Concurrency cho latency-sensitive Lambda

### High Availability

- [ ] Services deploy trÃªn â‰¥ 2 AZs
- [ ] Aurora Multi-AZ enabled
- [ ] ElastiCache Multi-AZ + automatic failover
- [ ] NAT Gateway per AZ
- [ ] ALB cross-zone load balancing enabled

### Resilience Patterns

- [ ] App Mesh outlier detection (Circuit Breaker) configured
- [ ] Retry policy cho má»—i service-to-service call
- [ ] Timeout configured (connection + request timeout)
- [ ] DLQ cho táº¥t cáº£ SQS queues
- [ ] Fallback strategy cho critical dependencies

### Health Check

- [ ] Container health check (liveness)
- [ ] ALB target group health check (readiness)
- [ ] Health check endpoint kiá»ƒm tra dependencies
- [ ] Startup grace period cho slow-starting services

### Disaster Recovery

- [ ] DR strategy Ä‘Ã£ chá»n (Pilot Light / Warm Standby / Active-Active)
- [ ] Database replication cross-region (Aurora Global / DynamoDB Global Tables)
- [ ] Route 53 failover routing configured
- [ ] DR runbook documented vÃ  tested
- [ ] Game Day scheduled hÃ ng quÃ½

### Chaos Engineering

- [ ] FIS experiment templates cho common failures
- [ ] Stop conditions defined (CloudWatch alarms)
- [ ] Chaos testing cháº¡y trÃªn staging thÆ°á»ng xuyÃªn
- [ ] Findings documented vÃ  action items tracked

---

## 11. Tá»•ng káº¿t

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             RESILIENCE DECISION GUIDE trÃªn AWS                    â”‚
â”‚                                                                   â”‚
â”‚  Auto Scaling:                                                    â”‚
â”‚  â€¢ ECS: Target Tracking (CPU 70%) + custom metric (SQS/RPS)       â”‚
â”‚  â€¢ EKS: HPA (Pod) + Karpenter (Node) + KEDA (event-driven)        â”‚
â”‚  â€¢ Lambda: Reserved + Provisioned Concurrency cho critical        â”‚
â”‚                                                                   â”‚
â”‚  High Availability:                                               â”‚
â”‚  â€¢ Minimum: Multi-AZ (99.99% SLA)                                 â”‚
â”‚  â€¢ Critical: Multi-Region (99.999% SLA)                           â”‚
â”‚  â€¢ Always: ALB + Health Check + Auto Scaling                      â”‚
â”‚                                                                   â”‚
â”‚  Resilience Patterns:                                             â”‚
â”‚  â€¢ Platform-level: App Mesh (CB, retry, timeout, connection pool) â”‚
â”‚  â€¢ Application-level: Custom fallback, graceful degradation       â”‚
â”‚  â€¢ Async: SQS + DLQ cho error isolation                           â”‚
â”‚  â€¢ Edge: WAF + API Gateway throttling cho rate limiting           â”‚
â”‚                                                                   â”‚
â”‚  Disaster Recovery:                                               â”‚
â”‚  â€¢ Non-critical: Backup & Restore ($)                             â”‚
â”‚  â€¢ Business apps: Pilot Light ($$)                                â”‚
â”‚  â€¢ Critical: Warm Standby ($$$)                                   â”‚
â”‚  â€¢ Mission-critical: Active-Active ($$$$)                         â”‚
â”‚                                                                   â”‚
â”‚  Chaos Engineering:                                               â”‚
â”‚  â€¢ AWS FIS cho managed experiments                                â”‚
â”‚  â€¢ Game Day hÃ ng quÃ½                                              â”‚
â”‚  â€¢ Start staging â†’ production (tÄƒng blast radius dáº§n)             â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key takeaways:**

1. **Auto Scaling lÃ  báº¯t buá»™c** â€” khÃ´ng manual scale trÃªn cloud, chá»n metric phÃ¹ há»£p workload
2. **Multi-AZ lÃ  minimum** â€” má»i production service pháº£i Multi-AZ, cost thÃªm ráº¥t Ã­t
3. **App Mesh cho platform-level resilience** â€” Circuit Breaker, retry, timeout khÃ´ng cáº§n code
4. **Health Check á»Ÿ má»i táº§ng** â€” Container, ALB, Route 53, má»—i táº§ng cÃ³ má»¥c Ä‘Ã­ch khÃ¡c nhau
5. **Chaos Engineering lÃ  vÄƒn hÃ³a** â€” khÃ´ng pháº£i one-time activity, cáº§n Game Day Ä‘á»‹nh ká»³
6. **DR pháº£i test thÆ°á»ng xuyÃªn** â€” DR plan chÆ°a test = khÃ´ng cÃ³ DR plan

---

## 12. LiÃªn káº¿t liÃªn quan

- [10 â€” Resilience Patterns](10-resilience-patterns.md) â€” LÃ½ thuyáº¿t Circuit Breaker, Retry, Bulkhead, Fallback
- [18 â€” Triá»ƒn khai & Kiáº¿n trÃºc tá»•ng quan](18-aws-deployment-architecture.md) â€” ECS vs EKS vs Lambda, IaC
- [19 â€” Communication & Service Discovery trÃªn AWS](19-aws-communication-discovery.md) â€” App Mesh, SQS/SNS, EventBridge
- [20 â€” Data Management trÃªn AWS](20-aws-data-management.md) â€” Aurora Global Database, DynamoDB Global Tables
- [22 â€” Observability trÃªn AWS](22-aws-observability.md) â€” Monitoring, alerting cho resilience
- [23 â€” Security trÃªn AWS](23-aws-security.md) â€” WAF, Shield, network isolation
- [25 â€” Case Study: E-Commerce](25-case-study-ecommerce.md) â€” Ãp dá»¥ng tá»•ng há»£p
