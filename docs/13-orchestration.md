# Orchestration â€” Äiá»u phá»‘i Container vá»›i Kubernetes

## ğŸ“‹ Má»¥c lá»¥c

- [1. Giá»›i thiá»‡u](#1-giá»›i-thiá»‡u)
- [2. Táº¡i sao cáº§n Orchestration?](#2-táº¡i-sao-cáº§n-orchestration)
  - [2.1. Váº¥n Ä‘á» khi cháº¡y container thá»§ cÃ´ng](#21-váº¥n-Ä‘á»-khi-cháº¡y-container-thá»§-cÃ´ng)
  - [2.2. Container Orchestration giáº£i quyáº¿t gÃ¬?](#22-container-orchestration-giáº£i-quyáº¿t-gÃ¬)
  - [2.3. CÃ¡c giáº£i phÃ¡p Orchestration](#23-cÃ¡c-giáº£i-phÃ¡p-orchestration)
- [3. Kubernetes â€” Kiáº¿n trÃºc tá»•ng quan](#3-kubernetes--kiáº¿n-trÃºc-tá»•ng-quan)
  - [3.1. Kubernetes Architecture](#31-kubernetes-architecture)
  - [3.2. Control Plane](#32-control-plane)
  - [3.3. Worker Node](#33-worker-node)
  - [3.4. Luá»“ng hoáº¡t Ä‘á»™ng khi deploy](#34-luá»“ng-hoáº¡t-Ä‘á»™ng-khi-deploy)
- [4. Kubernetes Objects â€” CÃ¡c thÃ nh pháº§n cá»‘t lÃµi](#4-kubernetes-objects--cÃ¡c-thÃ nh-pháº§n-cá»‘t-lÃµi)
  - [4.1. Pod](#41-pod)
  - [4.2. ReplicaSet](#42-replicaset)
  - [4.3. Deployment](#43-deployment)
  - [4.4. Service](#44-service)
  - [4.5. Ingress](#45-ingress)
  - [4.6. ConfigMap & Secret](#46-configmap--secret)
  - [4.7. Namespace](#47-namespace)
  - [4.8. Tá»•ng há»£p quan há»‡ giá»¯a cÃ¡c Objects](#48-tá»•ng-há»£p-quan-há»‡-giá»¯a-cÃ¡c-objects)
- [5. Scaling & Auto-scaling](#5-scaling--auto-scaling)
  - [5.1. Manual Scaling](#51-manual-scaling)
  - [5.2. Horizontal Pod Autoscaler (HPA)](#52-horizontal-pod-autoscaler-hpa)
  - [5.3. Vertical Pod Autoscaler (VPA)](#53-vertical-pod-autoscaler-vpa)
  - [5.4. Cluster Autoscaler](#54-cluster-autoscaler)
  - [5.5. KEDA â€” Event-Driven Autoscaling](#55-keda--event-driven-autoscaling)
- [6. Health Check & Self-Healing](#6-health-check--self-healing)
  - [6.1. Liveness Probe](#61-liveness-probe)
  - [6.2. Readiness Probe](#62-readiness-probe)
  - [6.3. Startup Probe](#63-startup-probe)
  - [6.4. So sÃ¡nh 3 loáº¡i Probe](#64-so-sÃ¡nh-3-loáº¡i-probe)
- [7. Resource Management](#7-resource-management)
  - [7.1. Requests & Limits](#71-requests--limits)
  - [7.2. QoS Classes](#72-qos-classes)
  - [7.3. LimitRange & ResourceQuota](#73-limitrange--resourcequota)
- [8. Helm â€” Package Manager cho Kubernetes](#8-helm--package-manager-cho-kubernetes)
  - [8.1. Helm lÃ  gÃ¬?](#81-helm-lÃ -gÃ¬)
  - [8.2. Helm Chart Structure](#82-helm-chart-structure)
  - [8.3. Helm Commands](#83-helm-commands)
  - [8.4. VÃ­ dá»¥ â€” Helm Chart cho Microservice](#84-vÃ­-dá»¥--helm-chart-cho-microservice)
- [9. Service Mesh](#9-service-mesh)
  - [9.1. Táº¡i sao cáº§n Service Mesh?](#91-táº¡i-sao-cáº§n-service-mesh)
  - [9.2. Service Mesh Architecture](#92-service-mesh-architecture)
  - [9.3. Istio](#93-istio)
  - [9.4. Linkerd](#94-linkerd)
  - [9.5. So sÃ¡nh Istio vs Linkerd](#95-so-sÃ¡nh-istio-vs-linkerd)
  - [9.6. Khi nÃ o cáº§n / khÃ´ng cáº§n Service Mesh?](#96-khi-nÃ o-cáº§n--khÃ´ng-cáº§n-service-mesh)
- [10. Kubernetes trÃªn Cloud](#10-kubernetes-trÃªn-cloud)
  - [10.1. Managed Kubernetes Services](#101-managed-kubernetes-services)
  - [10.2. Self-managed vs Managed](#102-self-managed-vs-managed)
- [11. VÃ­ dá»¥ thá»±c táº¿ â€” E-Commerce trÃªn Kubernetes](#11-vÃ­-dá»¥-thá»±c-táº¿--e-commerce-trÃªn-kubernetes)
- [12. Anti-patterns](#12-anti-patterns)
- [13. Checklist triá»ƒn khai](#13-checklist-triá»ƒn-khai)
- [14. Tá»•ng káº¿t](#14-tá»•ng-káº¿t)
- [15. LiÃªn káº¿t liÃªn quan](#15-liÃªn-káº¿t-liÃªn-quan)

---

## 1. Giá»›i thiá»‡u

Trong [doc 12](12-containerization.md), chÃºng ta Ä‘Ã£ Ä‘Ã³ng gÃ³i má»—i microservice thÃ nh Docker container. Docker Compose giÃºp cháº¡y multi-container trÃªn **1 mÃ¡y** cho development. NhÆ°ng production cáº§n cháº¡y trÃªn **nhiá»u mÃ¡y**, tá»± Ä‘á»™ng scale, tá»± phá»¥c há»“i khi crash â€” cáº§n má»™t há»‡ thá»‘ng **Ä‘iá»u phá»‘i (orchestration)**.

**Kubernetes** (K8s) lÃ  giáº£i phÃ¡p orchestration phá»• biáº¿n nháº¥t â€” quáº£n lÃ½ hÃ ng trÄƒm/ngÃ n containers trÃªn cluster nhiá»u mÃ¡y chá»§.

---

## 2. Táº¡i sao cáº§n Orchestration?

### 2.1. Váº¥n Ä‘á» khi cháº¡y container thá»§ cÃ´ng

```
âŒ CHáº Y CONTAINER THá»¦ CÃ”NG TRÃŠN PRODUCTION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  5 servers, 10 services, má»—i service 3 instances = 30 containers

  Server 1: user-svc (Ã—2), order-svc (Ã—1)
  Server 2: order-svc (Ã—2), product-svc (Ã—1)
  Server 3: product-svc (Ã—2), payment-svc (Ã—1)
  Server 4: payment-svc (Ã—2), notification (Ã—1)
  Server 5: notification (Ã—1), api-gateway (Ã—2)

  Váº¥n Ä‘á»:
  â”€â”€â”€â”€â”€â”€â”€â”€
  1. Container crash lÃºc 3AM
     â†’ Ai restart? Pháº£i SSH vÃ o server â†’ docker start
     â†’ KhÃ´ng ai biáº¿t cho Ä‘áº¿n khi user complain ğŸ˜±

  2. Server 3 háº¿t RAM
     â†’ Move container sang server khÃ¡c? Thá»§ cÃ´ng?
     â†’ Config láº¡i network, volumes? ğŸ˜°

  3. Deploy version má»›i user-service
     â†’ SSH vÃ o Server 1, stop container cÅ©, pull image má»›i, start
     â†’ Láº·p láº¡i cho Server 2... Server 3...
     â†’ Downtime trong lÃºc deploy? ğŸ˜«

  4. Black Friday â€” cáº§n scale order-service 3 â†’ 20 instances
     â†’ ThÃªm servers? Cháº¡y container á»Ÿ Ä‘Ã¢u? CÃ¢n báº±ng táº£i tháº¿ nÃ o?
     â†’ Sau Black Friday â†’ scale xuá»‘ng? Táº¯t servers? ğŸ¤¯

  5. Health check
     â†’ Container cháº¡y nhÆ°ng app bÃªn trong Ä‘Ã£ crash
     â†’ Docker khÃ´ng biáº¿t â†’ traffic váº«n gá»­i Ä‘áº¿n â†’ error 502
```

### 2.2. Container Orchestration giáº£i quyáº¿t gÃ¬?

```
âœ… CÃ“ ORCHESTRATION (Kubernetes):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Báº¡n nÃ³i: "TÃ´i muá»‘n 3 instances cá»§a order-service, luÃ´n cháº¡y"

  Kubernetes LO Táº¤T Cáº¢:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                            â”‚
  â”‚  1. SCHEDULING â€” Cháº¡y container á»Ÿ Ä‘Ã¢u?                     â”‚
  â”‚     K8s tá»± chá»n node cÃ³ Ä‘á»§ CPU/RAM â†’ Ä‘áº·t container         â”‚
  â”‚                                                            â”‚
  â”‚  2. SELF-HEALING â€” Container crash?                        â”‚
  â”‚     K8s phÃ¡t hiá»‡n â†’ tá»± restart hoáº·c táº¡o container má»›i      â”‚
  â”‚     â†’ 3AM cÅ©ng khÃ´ng cáº§n ai thá»©c dáº­y                       â”‚
  â”‚                                                            â”‚
  â”‚  3. SCALING â€” Cáº§n thÃªm instances?                          â”‚
  â”‚     kubectl scale deployment order-svc --replicas=20       â”‚
  â”‚     â†’ K8s táº¡o 17 containers má»›i, phÃ¢n bá»• lÃªn cÃ¡c nodes     â”‚
  â”‚     Hoáº·c auto-scale dá»±a trÃªn CPU/memory/custom metrics     â”‚
  â”‚                                                            â”‚
  â”‚  4. ROLLING UPDATE â€” Deploy version má»›i?                   â”‚
  â”‚     K8s tá»±: táº¡o container má»›i â†’ check healthy              â”‚
  â”‚     â†’ route traffic â†’ stop container cÅ© â†’ ZERO DOWNTIME    â”‚
  â”‚                                                            â”‚
  â”‚  5. SERVICE DISCOVERY â€” TÃ¬m service?                       â”‚
  â”‚     K8s DNS: http://order-service:8082                     â”‚
  â”‚     â†’ Tá»± resolve, tá»± load balance (xem doc 08)             â”‚
  â”‚                                                            â”‚
  â”‚  6. CONFIG MANAGEMENT â€” Cáº¥u hÃ¬nh?                          â”‚
  â”‚     ConfigMap, Secret â†’ inject vÃ o container               â”‚
  â”‚     â†’ Äá»•i config khÃ´ng cáº§n rebuild image                   â”‚
  â”‚                                                            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3. CÃ¡c giáº£i phÃ¡p Orchestration

| Giáº£i phÃ¡p | MÃ´ táº£ | Status |
|-----------|--------|--------|
| **Kubernetes (K8s)** | De facto standard, CNCF, Google | âœ… Dominant (>80% market) |
| **Docker Swarm** | Docker native, Ä‘Æ¡n giáº£n | âš ï¸ Ãt dÃ¹ng, Docker khuyáº¿n khÃ­ch dÃ¹ng K8s |
| **AWS ECS** | AWS managed, dÃ¹ng Task Definitions | âœ… Phá»• biáº¿n trÃªn AWS (khÃ´ng muá»‘n K8s) |
| **HashiCorp Nomad** | Lightweight, há»— trá»£ cáº£ non-container | âš ï¸ Niche, dÃ¹ng trong HashiCorp ecosystem |
| **Apache Mesos** | Distributed systems kernel | âŒ Deprecated (Twitter/X Ä‘Ã£ bá») |

> **Thá»±c táº¿**: Kubernetes chiáº¿m >80% thá»‹ trÆ°á»ng orchestration. Náº¿u khÃ´ng cÃ³ lÃ½ do Ä‘áº·c biá»‡t â†’ chá»n Kubernetes.

---

## 3. Kubernetes â€” Kiáº¿n trÃºc tá»•ng quan

### 3.1. Kubernetes Architecture

```
KUBERNETES CLUSTER ARCHITECTURE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  KUBERNETES CLUSTER                                             â”‚
  â”‚                                                                 â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
  â”‚  â”‚  CONTROL PLANE (Master)               â”‚                      â”‚
  â”‚  â”‚                                       â”‚                      â”‚
  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚
  â”‚  â”‚  â”‚ API Serverâ”‚  â”‚    Scheduler     â”‚  â”‚                      â”‚
  â”‚  â”‚  â”‚ (kube-    â”‚  â”‚ (chá»n node cho   â”‚  â”‚                      â”‚
  â”‚  â”‚  â”‚ apiserver)â”‚  â”‚  pod má»›i)        â”‚  â”‚                      â”‚
  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â”‚
  â”‚  â”‚        â”‚                              â”‚                      â”‚
  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚
  â”‚  â”‚  â”‚   etcd    â”‚  â”‚ Controller       â”‚  â”‚                      â”‚
  â”‚  â”‚  â”‚ (cluster  â”‚  â”‚ Manager          â”‚  â”‚                      â”‚
  â”‚  â”‚  â”‚  state)   â”‚  â”‚ (desired state   â”‚  â”‚                      â”‚
  â”‚  â”‚  â”‚           â”‚  â”‚  â†’ actual state) â”‚  â”‚                      â”‚
  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â”‚
  â”‚  â”‚                                       â”‚                      â”‚
  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚                      â”‚
  â”‚  â”‚  â”‚ Cloud Controller â”‚ â† (optional)    â”‚                      â”‚
  â”‚  â”‚  â”‚ Manager          â”‚   tÃ­ch há»£p      â”‚                      â”‚
  â”‚  â”‚  â”‚                  â”‚   cloud providerâ”‚                      â”‚
  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚                      â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
  â”‚                      â”‚                                          â”‚
  â”‚         kubectl â”€â”€â”€â”€â–¶â”‚â—€â”€â”€â”€â”€ K8s Dashboard                       â”‚
  â”‚                      â”‚                                          â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚                   â”‚    WORKER NODES                       â”‚  â”‚
  â”‚  â”‚                   â”‚                                       â”‚  â”‚
  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
  â”‚  â”‚  â”‚  Node 1                    â”‚  â”‚  Node 2              â”‚ â”‚  â”‚
  â”‚  â”‚  â”‚                            â”‚  â”‚                      â”‚ â”‚  â”‚
  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚ â”‚  â”‚
  â”‚  â”‚  â”‚  â”‚ Pod A  â”‚ â”‚ Pod B  â”‚     â”‚  â”‚  â”‚ Pod C  â”‚          â”‚ â”‚  â”‚
  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚ â”‚  â”‚
  â”‚  â”‚  â”‚                            â”‚  â”‚                      â”‚ â”‚  â”‚
  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”‚
  â”‚  â”‚  â”‚  â”‚ kubelet              â”‚  â”‚  â”‚  â”‚ kubelet        â”‚  â”‚ â”‚  â”‚
  â”‚  â”‚  â”‚  â”‚ (agent quáº£n lÃ½ pods) â”‚  â”‚  â”‚  â”‚                â”‚  â”‚ â”‚  â”‚
  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â”‚
  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”‚
  â”‚  â”‚  â”‚  â”‚ kube-proxy           â”‚  â”‚  â”‚  â”‚ kube-proxy     â”‚  â”‚ â”‚  â”‚
  â”‚  â”‚  â”‚  â”‚ (network routing)    â”‚  â”‚  â”‚  â”‚                â”‚  â”‚ â”‚  â”‚
  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â”‚
  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”‚
  â”‚  â”‚  â”‚  â”‚ Container Runtime    â”‚  â”‚  â”‚  â”‚ Container      â”‚  â”‚ â”‚  â”‚
  â”‚  â”‚  â”‚  â”‚ (containerd)         â”‚  â”‚  â”‚  â”‚ Runtime        â”‚  â”‚ â”‚  â”‚
  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â”‚
  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2. Control Plane

Control Plane lÃ  **"bá»™ nÃ£o"** cá»§a cluster â€” quyáº¿t Ä‘á»‹nh má»i thá»©.

| Component | Vai trÃ² | VÃ­ dá»¥ |
|-----------|---------|-------|
| **API Server** | Cá»•ng vÃ o duy nháº¥t cá»§a cluster. Má»i request (kubectl, dashboard, controllers) Ä‘á»u qua API Server | `kubectl get pods` â†’ API Server â†’ tráº£ vá» danh sÃ¡ch pods |
| **etcd** | Distributed key-value store â€” lÆ°u **toÃ n bá»™ cluster state** | Pods nÃ o Ä‘ang cháº¡y, trÃªn node nÃ o, config gÃ¬ |
| **Scheduler** | Chá»n node phÃ¹ há»£p cho pod má»›i dá»±a trÃªn resources, affinity, taints | Pod cáº§n 2GB RAM â†’ Scheduler chá»n Node 2 (cÃ²n 4GB free) |
| **Controller Manager** | Cháº¡y cÃ¡c controllers Ä‘áº£m báº£o **desired state = actual state** | ReplicaSet controller: "cáº§n 3 pods, hiá»‡n cÃ³ 2 â†’ táº¡o thÃªm 1" |
| **Cloud Controller Manager** | TÃ­ch há»£p cloud provider (AWS, GCP, Azure) | Táº¡o LoadBalancer â†’ Cloud Controller táº¡o ALB trÃªn AWS |

### 3.3. Worker Node

Worker Node lÃ  **mÃ¡y cháº¡y containers** thá»±c sá»±.

| Component | Vai trÃ² |
|-----------|---------|
| **kubelet** | Agent trÃªn má»—i node. Nháº­n lá»‡nh tá»« API Server â†’ quáº£n lÃ½ pods (start/stop/health check) |
| **kube-proxy** | Quáº£n lÃ½ network rules. Route traffic Ä‘áº¿n Ä‘Ãºng pod (iptables/IPVS) |
| **Container Runtime** | Cháº¡y containers (containerd, CRI-O). Xem [doc 12](12-containerization.md) |

### 3.4. Luá»“ng hoáº¡t Ä‘á»™ng khi deploy

```
DEPLOY FLOW â€” kubectl apply -f deployment.yaml:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Developer
      â”‚
      â”‚  kubectl apply -f order-deployment.yaml
      â”‚  "TÃ´i muá»‘n 3 pods order-service:2.0"
      â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚API Serverâ”‚  1. Validate YAML â†’ lÆ°u vÃ o etcd
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚Controllerâ”‚  2. Deployment Controller tháº¥y: desired=3, actual=0
  â”‚ Manager  â”‚     â†’ Táº¡o ReplicaSet â†’ ReplicaSet táº¡o 3 Pod objects
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚Scheduler â”‚  3. Cho má»—i Pod (chÆ°a cÃ³ node):
  â”‚          â”‚     â†’ TÃ¬m node phÃ¹ há»£p (CPU/RAM/affinity)
  â”‚          â”‚     â†’ Assign: Pod 1â†’Node 1, Pod 2â†’Node 2, Pod 3â†’Node 1
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼                    â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ kubelet  â”‚         â”‚ kubelet  â”‚
  â”‚ (Node 1) â”‚         â”‚ (Node 2) â”‚
  â”‚          â”‚         â”‚          â”‚
  â”‚ 4. Pull  â”‚         â”‚ 4. Pull  â”‚
  â”‚  image   â”‚         â”‚  image   â”‚
  â”‚ 5. Start â”‚         â”‚ 5. Start â”‚
  â”‚ containerâ”‚         â”‚ containerâ”‚
  â”‚ 6. Healthâ”‚         â”‚ 6. Healthâ”‚
  â”‚  check âœ“ â”‚         â”‚  check âœ“ â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  7. kube-proxy cáº­p nháº­t routing rules
     â†’ traffic Ä‘áº¿n order-service â†’ load balance giá»¯a 3 pods
  8. Pods sáºµn sÃ ng nháº­n traffic âœ…
```

---

## 4. Kubernetes Objects â€” CÃ¡c thÃ nh pháº§n cá»‘t lÃµi

### 4.1. Pod

Pod lÃ  **Ä‘Æ¡n vá»‹ nhá» nháº¥t** trong Kubernetes â€” chá»©a 1 hoáº·c nhiá»u containers chia sáº» network vÃ  storage.

```
POD:
â”€â”€â”€â”€

  ThÃ´ng thÆ°á»ng: 1 Pod = 1 Container (main application)

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Pod: order-service-7d4f8b-x2k   â”‚
  â”‚                                  â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚  Container: order-service  â”‚  â”‚
  â”‚  â”‚  Image: order-service:2.0  â”‚  â”‚
  â”‚  â”‚  Port: 8082                â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â”‚                                  â”‚
  â”‚  IP: 10.244.1.15                 â”‚
  â”‚  (má»—i Pod cÃ³ 1 IP riÃªng)         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


  Multi-container Pod (Sidecar pattern):

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Pod: order-service-7d4f8b-x2k           â”‚
  â”‚                                          â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚  Container:      â”‚ â”‚  Sidecar:     â”‚  â”‚
  â”‚  â”‚  order-service   â”‚ â”‚  log-shipper  â”‚  â”‚
  â”‚  â”‚  (main app)      â”‚ â”‚  (Fluent Bit) â”‚  â”‚
  â”‚  â”‚  Port: 8082      â”‚ â”‚               â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â”‚           â”‚                   â”‚          â”‚
  â”‚           â””â”€â”€â”€â”€â”€ shared â”€â”€â”€â”€â”€â”€â”˜          â”‚
  â”‚            network (localhost)           â”‚
  â”‚            volumes (/var/log)            â”‚
  â”‚                                          â”‚
  â”‚  IP: 10.244.1.15                         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Containers trong cÃ¹ng Pod:
  â€¢ Chia sáº» network â†’ gá»i nhau qua localhost
  â€¢ Chia sáº» volumes â†’ Ä‘á»c/ghi cÃ¹ng files
  â€¢ CÃ¹ng lifecycle â†’ start/stop cÃ¹ng nhau
```

```yaml
# pod.yaml â€” Ãt khi táº¡o Pod trá»±c tiáº¿p, thÆ°á»ng dÃ¹ng Deployment
apiVersion: v1
kind: Pod
metadata:
  name: order-service
  labels:
    app: order-service
    version: "2.0"
spec:
  containers:
    - name: order-service
      image: order-service:2.0
      ports:
        - containerPort: 8082
      env:
        - name: DB_HOST
          value: "postgres"
      resources:
        requests:
          cpu: "250m"
          memory: "256Mi"
        limits:
          cpu: "500m"
          memory: "512Mi"
```

### 4.2. ReplicaSet

ReplicaSet Ä‘áº£m báº£o **luÃ´n cÃ³ Ä‘Ãºng N pods** Ä‘ang cháº¡y.

```
REPLICASET:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  ReplicaSet: order-service-rs  (replicas: 3)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                      â”‚
  â”‚  Desired: 3 pods      Actual: 3 pods     âœ… OK       â”‚
  â”‚                                                      â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
  â”‚  â”‚  Pod 1   â”‚  â”‚  Pod 2   â”‚  â”‚  Pod 3   â”‚            â”‚
  â”‚  â”‚  order   â”‚  â”‚  order   â”‚  â”‚  order   â”‚            â”‚
  â”‚  â”‚  :2.0    â”‚  â”‚  :2.0    â”‚  â”‚  :2.0    â”‚            â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
  â”‚                                                      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Pod 2 crash:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                      â”‚
  â”‚  Desired: 3 pods      Actual: 2 pods     âŒ Mismatch â”‚
  â”‚                                                      â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
  â”‚  â”‚  Pod 1   â”‚  â”‚  Pod 3   â”‚  Pod 2 crashed!          â”‚
  â”‚  â”‚  order   â”‚  â”‚  order   â”‚                          â”‚
  â”‚  â”‚  :2.0    â”‚  â”‚  :2.0    â”‚                          â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
  â”‚                                                      â”‚
  â”‚  â†’ ReplicaSet táº¡o Pod 4 ngay láº­p tá»©c:                â”‚
  â”‚                                                      â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
  â”‚  â”‚  Pod 1   â”‚  â”‚  Pod 3   â”‚  â”‚  Pod 4   â”‚  â† Má»šI     â”‚
  â”‚  â”‚  order   â”‚  â”‚  order   â”‚  â”‚  order   â”‚            â”‚
  â”‚  â”‚  :2.0    â”‚  â”‚  :2.0    â”‚  â”‚  :2.0    â”‚            â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
  â”‚                                                      â”‚
  â”‚  Desired: 3       Actual: 3             âœ… OK        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  âš ï¸ Thá»±c táº¿: KHÃ”NG táº¡o ReplicaSet trá»±c tiáº¿p.
     DÃ¹ng Deployment â†’ Deployment táº¡o ReplicaSet cho báº¡n.
```

### 4.3. Deployment

Deployment quáº£n lÃ½ **ReplicaSets** vÃ  há»— trá»£ **rolling updates**, **rollback**.

```yaml
# deployment.yaml â€” Object dÃ¹ng NHIá»€U NHáº¤T trong K8s
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service
  namespace: production
  labels:
    app: order-service
spec:
  replicas: 3                          # Sá»‘ pods mong muá»‘n
  selector:
    matchLabels:
      app: order-service               # Pods nÃ o thuá»™c Deployment nÃ y
  strategy:
    type: RollingUpdate                # Chiáº¿n lÆ°á»£c update
    rollingUpdate:
      maxSurge: 1                      # Tá»‘i Ä‘a thÃªm 1 pod khi update
      maxUnavailable: 0                # KhÃ´ng cho phÃ©p pod nÃ o down
  template:                            # Pod template
    metadata:
      labels:
        app: order-service
        version: "2.0"
    spec:
      containers:
        - name: order-service
          image: order-service:2.0.0
          ports:
            - containerPort: 8082
          env:
            - name: DB_HOST
              valueFrom:
                configMapKeyRef:
                  name: order-config
                  key: db-host
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: order-secrets
                  key: db-password
          resources:
            requests:
              cpu: "250m"
              memory: "256Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"
          livenessProbe:
            httpGet:
              path: /actuator/health/liveness
              port: 8082
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /actuator/health/readiness
              port: 8082
            initialDelaySeconds: 10
            periodSeconds: 5
```

```
ROLLING UPDATE â€” Zero Downtime Deploy:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Hiá»‡n táº¡i: order-service:1.0 (3 pods)
  Má»¥c tiÃªu: order-service:2.0

  kubectl set image deployment/order-service order-service=order-service:2.0

  Step 1: Táº¡o pod má»›i v2.0
  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚ v1.0 â”‚  â”‚ v1.0 â”‚  â”‚ v1.0 â”‚  â”‚ v2.0 â”‚ â† má»›i, chá» healthy
  â”‚  âœ…  â”‚  â”‚  âœ…  â”‚  â”‚  âœ…  â”‚  â”‚  â³  â”‚
  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜

  Step 2: v2.0 healthy â†’ xÃ³a 1 v1.0
  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚ v1.0 â”‚  â”‚ v1.0 â”‚  â”‚ v2.0 â”‚
  â”‚  âœ…  â”‚  â”‚  âœ…  â”‚  â”‚  âœ…  â”‚
  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜

  Step 3: Táº¡o thÃªm v2.0, xÃ³a v1.0
  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚ v1.0 â”‚  â”‚ v2.0 â”‚  â”‚ v2.0 â”‚
  â”‚  âœ…  â”‚  â”‚  âœ…  â”‚  â”‚  âœ…  â”‚
  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜

  Step 4: HoÃ n thÃ nh
  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚ v2.0 â”‚  â”‚ v2.0 â”‚  â”‚ v2.0 â”‚
  â”‚  âœ…  â”‚  â”‚  âœ…  â”‚  â”‚  âœ…  â”‚
  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜

  â†’ LuÃ´n cÃ³ pods serving â†’ ZERO DOWNTIME âœ…

  Rollback náº¿u v2.0 cÃ³ bug:
  kubectl rollout undo deployment/order-service
  â†’ K8s tá»± revert vá» v1.0
```

### 4.4. Service

Service cung cáº¥p **stable endpoint** Ä‘á»ƒ truy cáº­p pods. Pods cÃ³ thá»ƒ bá»‹ táº¡o/xÃ³a liÃªn tá»¥c (IP thay Ä‘á»•i), nhÆ°ng Service IP vÃ  DNS name **khÃ´ng Ä‘á»•i**.

```
KUBERNETES SERVICE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Pods thay Ä‘á»•i liÃªn tá»¥c:
    Pod order-abc (IP 10.244.1.5)  â†’ bá»‹ xÃ³a
    Pod order-def (IP 10.244.2.8)  â†’ Ä‘Æ°á»£c táº¡o má»›i
    â†’ IP thay Ä‘á»•i liÃªn tá»¥c â†’ client khÃ´ng thá»ƒ hardcode IP

  Service giáº£i quyáº¿t:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Service: order-service                              â”‚
  â”‚  ClusterIP: 10.96.32.100  (KHÃ”NG Äá»”I)                â”‚
  â”‚  DNS: order-service.production.svc.cluster.local     â”‚
  â”‚                                                      â”‚
  â”‚  selector: app=order-service                         â”‚
  â”‚                                                      â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
  â”‚  â”‚  Pod 1     â”‚  â”‚  Pod 2     â”‚  â”‚  Pod 3     â”‚      â”‚
  â”‚  â”‚ 10.244.1.15â”‚  â”‚ 10.244.2.20â”‚  â”‚ 10.244.1.30â”‚      â”‚
  â”‚  â”‚            â”‚  â”‚            â”‚  â”‚            â”‚      â”‚
  â”‚  â”‚ app:       â”‚  â”‚ app:       â”‚  â”‚ app:       â”‚      â”‚
  â”‚  â”‚ order-svc  â”‚  â”‚ order-svc  â”‚  â”‚ order-svc  â”‚      â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
  â”‚                                                      â”‚
  â”‚  Client gá»i: http://order-service:8082               â”‚
  â”‚  â†’ ClusterIP 10.96.32.100                            â”‚
  â”‚  â†’ kube-proxy route â†’ 1 trong 3 pods                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```yaml
# service.yaml
apiVersion: v1
kind: Service
metadata:
  name: order-service
  namespace: production
spec:
  type: ClusterIP               # Internal only (default)
  selector:
    app: order-service           # Route Ä‘áº¿n pods cÃ³ label nÃ y
  ports:
    - port: 8082                 # Service port
      targetPort: 8082           # Container port
      protocol: TCP
```

```
SERVICE TYPES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  1. ClusterIP (default) â€” Internal only:
     â†’ Chá»‰ pods trong cluster gá»i Ä‘Æ°á»£c
     â†’ DÃ¹ng cho: service-to-service communication

  2. NodePort â€” Expose qua port trÃªn node:
     â†’ Má»Ÿ port 30000-32767 trÃªn Má»ŒI node
     â†’ External: http://<node-ip>:30080
     â†’ DÃ¹ng cho: dev/test, bare-metal

  3. LoadBalancer â€” Cloud Load Balancer:
     â†’ K8s yÃªu cáº§u cloud provider táº¡o LB (ALB/NLB)
     â†’ External: http://<lb-dns>
     â†’ DÃ¹ng cho: expose service ra internet (má»—i service = 1 LB â†’ Ä‘áº¯t)

  4. ExternalName â€” DNS alias:
     â†’ Map service name â†’ external DNS
     â†’ DÃ¹ng cho: trá» Ä‘áº¿n service ngoÃ i cluster (RDS, external API)
```

### 4.5. Ingress

Ingress quáº£n lÃ½ **HTTP/HTTPS routing** tá»« bÃªn ngoÃ i vÃ o cluster â€” thay vÃ¬ táº¡o LoadBalancer cho má»—i service.

```
INGRESS:
â”€â”€â”€â”€â”€â”€â”€â”€

  KhÃ´ng cÃ³ Ingress â€” 1 LoadBalancer per service ($$$$):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                      â”‚
  â”‚  ALB ($18/mo) â†’ user-service (ClusterIP)             â”‚
  â”‚  ALB ($18/mo) â†’ order-service (ClusterIP)            â”‚
  â”‚  ALB ($18/mo) â†’ product-service (ClusterIP)          â”‚
  â”‚                                                      â”‚
  â”‚  3 services = 3 ALBs = $54/month + data transfer     â”‚
  â”‚  10 services = $180/month ğŸ˜±                         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  CÃ³ Ingress â€” 1 LoadBalancer, routing theo path:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                      â”‚
  â”‚  ALB ($18/mo) â†’ Ingress Controller (NGINX/Kong)      â”‚
  â”‚                       â”‚                              â”‚
  â”‚                  /users   â†’ user-service             â”‚
  â”‚                  /orders  â†’ order-service            â”‚
  â”‚                  /productsâ†’ product-service          â”‚
  â”‚                                                      â”‚
  â”‚  10 services = 1 ALB = $18/month âœ…                  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```yaml
# ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: api-ingress
  namespace: production
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: letsencrypt-prod
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - api.myapp.com
      secretName: api-tls
  rules:
    - host: api.myapp.com
      http:
        paths:
          - path: /users
            pathType: Prefix
            backend:
              service:
                name: user-service
                port:
                  number: 8081
          - path: /orders
            pathType: Prefix
            backend:
              service:
                name: order-service
                port:
                  number: 8082
          - path: /products
            pathType: Prefix
            backend:
              service:
                name: product-service
                port:
                  number: 8083
```

### 4.6. ConfigMap & Secret

```
CONFIGMAP â€” LÆ°u config khÃ´ng nháº¡y cáº£m:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Táº¡i sao?
  Config trong image â†’ Ä‘á»•i config = rebuild image â†’ cháº­m, lÃ£ng phÃ­
  ConfigMap â†’ Ä‘á»•i config khÃ´ng cáº§n rebuild image
```

```yaml
# configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: order-config
  namespace: production
data:
  db-host: "postgres.production.svc"
  db-port: "5432"
  kafka-brokers: "kafka-0:9092,kafka-1:9092,kafka-2:9092"
  log-level: "INFO"
  cache-ttl: "300"
```

```
SECRET â€” LÆ°u data nháº¡y cáº£m (passwords, tokens, keys):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  âš ï¸ K8s Secret máº·c Ä‘á»‹nh chá»‰ encode base64 (KHÃ”NG encrypt!)
  â†’ Cáº§n káº¿t há»£p: Sealed Secrets, External Secrets, Vault
```

```yaml
# secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: order-secrets
  namespace: production
type: Opaque
data:
  db-password: cGFzc3dvcmQxMjM=    # base64 encoded "password123"
  api-key: YWJjZGVmZzEyMzQ1        # base64 encoded
```

```yaml
# Sá»­ dá»¥ng ConfigMap & Secret trong Deployment:
spec:
  containers:
    - name: order-service
      env:
        # Tá»« ConfigMap
        - name: DB_HOST
          valueFrom:
            configMapKeyRef:
              name: order-config
              key: db-host
        # Tá»« Secret
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: order-secrets
              key: db-password
      # Hoáº·c mount cáº£ ConfigMap thÃ nh file:
      volumeMounts:
        - name: config-volume
          mountPath: /app/config
  volumes:
    - name: config-volume
      configMap:
        name: order-config
```

### 4.7. Namespace

Namespace phÃ¢n **chia tÃ i nguyÃªn** trong cluster â€” giá»‘ng nhÆ° "thÆ° má»¥c" cho resources.

```
NAMESPACE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Kubernetes Cluster                                  â”‚
  â”‚                                                      â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚  Namespace: productionâ”‚ â”‚  Namespace: staging  â”‚  â”‚
  â”‚  â”‚                      â”‚  â”‚                      â”‚  â”‚
  â”‚  â”‚  order-service (Ã—3)  â”‚  â”‚  order-service (Ã—1)  â”‚  â”‚
  â”‚  â”‚  user-service  (Ã—3)  â”‚  â”‚  user-service  (Ã—1)  â”‚  â”‚
  â”‚  â”‚  payment-svc   (Ã—3)  â”‚  â”‚  payment-svc   (Ã—1)  â”‚  â”‚
  â”‚  â”‚                      â”‚  â”‚                      â”‚  â”‚
  â”‚  â”‚  ResourceQuota:      â”‚  â”‚  ResourceQuota:      â”‚  â”‚
  â”‚  â”‚  CPU: 20 cores       â”‚  â”‚  CPU: 4 cores        â”‚  â”‚
  â”‚  â”‚  RAM: 40 Gi          â”‚  â”‚  RAM: 8 Gi           â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â”‚                                                      â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚  Namespace: infra    â”‚  â”‚  Namespace: monitoringâ”‚ â”‚
  â”‚  â”‚                      â”‚  â”‚                      â”‚  â”‚
  â”‚  â”‚  postgres            â”‚  â”‚  prometheus          â”‚  â”‚
  â”‚  â”‚  redis               â”‚  â”‚  grafana             â”‚  â”‚
  â”‚  â”‚  kafka               â”‚  â”‚  jaeger              â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Cross-namespace communication:
  order-service (production) â†’ postgres (infra):
  http://postgres.infra.svc.cluster.local:5432
```

### 4.8. Tá»•ng há»£p quan há»‡ giá»¯a cÃ¡c Objects

```mermaid
graph TD
    NS[Namespace] --> DEP[Deployment]
    NS --> SVC[Service]
    NS --> ING[Ingress]
    NS --> CM[ConfigMap]
    NS --> SEC[Secret]
    
    DEP --> RS[ReplicaSet]
    RS --> POD1[Pod 1]
    RS --> POD2[Pod 2]
    RS --> POD3[Pod 3]
    
    SVC -->|selector: app=order| POD1
    SVC -->|selector: app=order| POD2
    SVC -->|selector: app=order| POD3
    
    ING -->|/orders| SVC
    
    CM -->|env vars| POD1
    SEC -->|env vars| POD1
```

---

## 5. Scaling & Auto-scaling

### 5.1. Manual Scaling

```bash
# Scale thá»§ cÃ´ng
kubectl scale deployment order-service --replicas=5

# Hoáº·c edit deployment
kubectl edit deployment order-service
# â†’ Sá»­a spec.replicas: 5
```

### 5.2. Horizontal Pod Autoscaler (HPA)

HPA tá»± Ä‘á»™ng **tÄƒng/giáº£m sá»‘ pods** dá»±a trÃªn metrics (CPU, memory, custom).

```yaml
# hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: order-service-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: order-service
  minReplicas: 3                    # Tá»‘i thiá»ƒu 3 pods
  maxReplicas: 20                   # Tá»‘i Ä‘a 20 pods
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70    # Scale khi CPU > 70%
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80    # Scale khi Memory > 80%
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60    # Chá» 60s trÆ°á»›c khi scale up
      policies:
        - type: Pods
          value: 4                      # ThÃªm tá»‘i Ä‘a 4 pods má»—i láº§n
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300   # Chá» 5 phÃºt trÆ°á»›c khi scale down
      policies:
        - type: Pods
          value: 1                      # Giáº£m tá»‘i Ä‘a 1 pod má»—i láº§n
          periodSeconds: 60
```

```
HPA HOáº T Äá»˜NG:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  CPU Target: 70%

  10:00  CPU: 30%  â†’ 3 pods (min)
  10:05  CPU: 65%  â†’ 3 pods (chÆ°a Ä‘áº¿n 70%)
  10:10  CPU: 85%  â†’ Scale up! â†’ 5 pods
  10:15  CPU: 90%  â†’ Scale up! â†’ 8 pods
  10:20  CPU: 60%  â†’ Giá»¯ 8 pods (stabilization window)
  10:30  CPU: 40%  â†’ Scale down â†’ 7 pods
  10:35  CPU: 35%  â†’ Scale down â†’ 6 pods
  ...
  11:00  CPU: 25%  â†’ Scale down â†’ 3 pods (min)
```

### 5.3. Vertical Pod Autoscaler (VPA)

VPA tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh **CPU/memory requests vÃ  limits** cho pods.

```
VPA:
â”€â”€â”€â”€
  Thay vÃ¬ thÃªm pods (HPA) â†’ VPA tÄƒng resources cho pod hiá»‡n táº¡i.

  Ban Ä‘áº§u:  Pod order-service: CPU 250m, Memory 256Mi
  VPA phÃ¢n tÃ­ch â†’ thá»±c táº¿ cáº§n CPU 500m, Memory 512Mi
  â†’ VPA restart pod vá»›i resources má»›i

  âš ï¸ VPA vÃ  HPA KHÃ”NG NÃŠN dÃ¹ng cÃ¹ng metric (vÃ­ dá»¥ cáº£ hai theo CPU)
  â†’ Giáº£i phÃ¡p: HPA theo CPU, VPA theo memory (hoáº·c chá»‰ dÃ¹ng 1)
```

### 5.4. Cluster Autoscaler

Cluster Autoscaler tá»± Ä‘á»™ng **thÃªm/bá»›t nodes** (mÃ¡y chá»§) trong cluster.

```
CLUSTER AUTOSCALER:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  1. HPA muá»‘n scale order-service 3 â†’ 20 pods
  2. Cluster hiá»‡n táº¡i chá»‰ cÃ³ 3 nodes, khÃ´ng Ä‘á»§ resources
  3. Scheduler: "Pods pending â€” khÃ´ng cÃ³ node phÃ¹ há»£p"
  4. Cluster Autoscaler phÃ¡t hiá»‡n â†’ thÃªm nodes:

  TrÆ°á»›c:                          Sau:
  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚Node 1â”‚ â”‚Node 2â”‚ â”‚Node 3â”‚     â”‚Node 1â”‚ â”‚Node 2â”‚ â”‚Node 3â”‚ â”‚Node 4â”‚ â”‚Node 5â”‚
  â”‚ FULL â”‚ â”‚ FULL â”‚ â”‚ FULL â”‚     â”‚ FULL â”‚ â”‚ FULL â”‚ â”‚ FULL â”‚ â”‚ pods â”‚ â”‚ pods â”‚
  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
                                                              â†‘ NEW    â†‘ NEW

  Khi traffic giáº£m â†’ HPA scale down pods â†’ nodes trá»‘ng
  â†’ Cluster Autoscaler xÃ³a nodes trá»‘ng â†’ tiáº¿t kiá»‡m tiá»n ğŸ’°
```

### 5.5. KEDA â€” Event-Driven Autoscaling

KEDA (Kubernetes Event-Driven Autoscaling) scale dá»±a trÃªn **event sources** (Kafka lag, SQS queue length, Prometheus metrics...).

```
KEDA:
â”€â”€â”€â”€â”€

  HPA chá»‰ scale theo CPU/memory â†’ khÃ´ng Ä‘á»§ cho event-driven systems

  KEDA má»Ÿ rá»™ng: scale theo Báº¤T Ká»² metric nÃ o:
  â€¢ Kafka: consumer lag > 1000 messages â†’ scale up consumers
  â€¢ SQS:   queue depth > 100 â†’ scale up workers
  â€¢ Cron:  9AM-6PM â†’ scale up, nights â†’ scale to 0
  â€¢ Prometheus: custom metric (request rate, error rate)

  Äáº·c biá»‡t: KEDA cÃ³ thá»ƒ scale TO ZERO (0 pods)
  â†’ HPA chá»‰ scale tá»‘i thiá»ƒu 1 pod
  â†’ KEDA: khÃ´ng cÃ³ events â†’ 0 pods â†’ tiáº¿t kiá»‡m resources
```

---

## 6. Health Check & Self-Healing

### 6.1. Liveness Probe

**"Container cÃ²n sá»‘ng khÃ´ng?"** â€” Náº¿u fail â†’ K8s **restart** container.

```yaml
livenessProbe:
  httpGet:
    path: /actuator/health/liveness
    port: 8082
  initialDelaySeconds: 30      # Chá» 30s sau khi start
  periodSeconds: 10            # Check má»—i 10s
  timeoutSeconds: 3            # Timeout má»—i láº§n check
  failureThreshold: 3          # Fail 3 láº§n â†’ restart
```

```
Khi nÃ o liveness probe cáº§n?
  â†’ App bá»‹ deadlock (thread stuck, khÃ´ng pháº£n há»“i)
  â†’ Memory leak â†’ OOM nhÆ°ng process chÆ°a crash
  â†’ App vÃ o tráº¡ng thÃ¡i lá»—i khÃ´ng thá»ƒ recover
  â†’ Container cháº¡y nhÆ°ng app bÃªn trong Ä‘Ã£ "cháº¿t"
```

### 6.2. Readiness Probe

**"Container sáºµn sÃ ng nháº­n traffic chÆ°a?"** â€” Náº¿u fail â†’ K8s **ngÆ°ng gá»­i traffic** (nhÆ°ng KHÃ”NG restart).

```yaml
readinessProbe:
  httpGet:
    path: /actuator/health/readiness
    port: 8082
  initialDelaySeconds: 10      # Chá» 10s
  periodSeconds: 5             # Check má»—i 5s
  failureThreshold: 3          # Fail 3 láº§n â†’ loáº¡i khá»i Service endpoints
```

```
Khi nÃ o readiness probe fail?
  â†’ App Ä‘ang khá»Ÿi Ä‘á»™ng (loading cache, warming up)
  â†’ DB connection chÆ°a sáºµn sÃ ng
  â†’ Downstream dependency Ä‘ang down
  â†’ App Ä‘ang overloaded (muá»‘n táº¡m ngÆ°ng nháº­n traffic)

  Traffic flow:
  Service â†’ Pod 1 âœ… (ready)
          â†’ Pod 2 âŒ (not ready â†’ KHÃ”NG gá»­i traffic)
          â†’ Pod 3 âœ… (ready)
```

### 6.3. Startup Probe

**"App Ä‘Ã£ khá»Ÿi Ä‘á»™ng xong chÆ°a?"** â€” DÃ¹ng cho app **khá»Ÿi Ä‘á»™ng cháº­m** (Java, large cache).

```yaml
startupProbe:
  httpGet:
    path: /actuator/health
    port: 8082
  initialDelaySeconds: 0
  periodSeconds: 10
  failureThreshold: 30         # 30 Ã— 10s = 5 phÃºt Ä‘á»ƒ start
```

```
Táº¡i sao cáº§n Startup Probe?
  KhÃ´ng cÃ³ startup probe:
    â†’ liveness probe check ngay â†’ app chÆ°a start xong â†’ fail
    â†’ K8s restart â†’ app láº¡i chÆ°a start xong â†’ fail â†’ restart loop ğŸ’€

  CÃ³ startup probe:
    â†’ K8s chá» startup probe pass (tá»‘i Ä‘a 5 phÃºt)
    â†’ Sau khi pass â†’ liveness + readiness probe má»›i báº¯t Ä‘áº§u
```

### 6.4. So sÃ¡nh 3 loáº¡i Probe

| Probe | CÃ¢u há»i | Khi fail | Khi nÃ o dÃ¹ng |
|-------|---------|----------|--------------|
| **Startup** | "App Ä‘Ã£ start xong chÆ°a?" | Chá» tiáº¿p (Ä‘áº¿n failureThreshold) | App khá»Ÿi Ä‘á»™ng cháº­m (Java, cache) |
| **Liveness** | "App cÃ²n sá»‘ng khÃ´ng?" | **Restart** container | PhÃ¡t hiá»‡n deadlock, unrecoverable state |
| **Readiness** | "App sáºµn sÃ ng nháº­n traffic?" | **Loáº¡i khá»i** Service endpoints | DB chÆ°a sáºµn sÃ ng, Ä‘ang overloaded |

```
Probe Timeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Container start
      â”‚
      â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Startup Probe   â”‚  "ÄÃ£ start xong chÆ°a?"
  â”‚  Check má»—i 10s   â”‚
  â”‚  Tá»‘i Ä‘a 5 phÃºt   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ PASS âœ…
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Liveness Probe  â”‚  â”‚  Readiness Probe â”‚
  â”‚  Check má»—i 10s   â”‚  â”‚  Check má»—i 5s    â”‚
  â”‚  "CÃ²n sá»‘ng?"     â”‚  â”‚  "Sáºµn sÃ ng?"     â”‚
  â”‚                  â”‚  â”‚                  â”‚
  â”‚  Fail â†’ RESTART  â”‚  â”‚ Fail â†’ NO TRAFFICâ”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. Resource Management

### 7.1. Requests & Limits

```
REQUESTS VÃ€ LIMITS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  requests: LÆ°á»£ng resources Tá»I THIá»‚U K8s Ä‘áº£m báº£o cho pod
            â†’ Scheduler dÃ¹ng Ä‘á»ƒ chá»n node
  limits:   LÆ°á»£ng resources Tá»I ÄA pod Ä‘Æ°á»£c phÃ©p dÃ¹ng
            â†’ VÆ°á»£t CPU limit â†’ throttle
            â†’ VÆ°á»£t Memory limit â†’ OOMKilled (bá»‹ kill)

  VÃ­ dá»¥:
  resources:
    requests:
      cpu: "250m"        # 0.25 CPU core â€” tá»‘i thiá»ƒu
      memory: "256Mi"    # 256 MB RAM â€” tá»‘i thiá»ƒu
    limits:
      cpu: "500m"        # 0.5 CPU core â€” tá»‘i Ä‘a
      memory: "512Mi"    # 512 MB RAM â€” tá»‘i Ä‘a (vÆ°á»£t â†’ OOMKilled)

  ÄÆ¡n vá»‹:
  â”€â”€â”€â”€â”€â”€â”€â”€
  CPU:    "1"    = 1 core
          "500m" = 0.5 core (milli-core)
          "250m" = 0.25 core

  Memory: "128Mi" = 128 MiB (mebibyte)
          "1Gi"   = 1 GiB (gibibyte)

  âš ï¸ LuÃ´n set cáº£ requests VÃ€ limits:
  â€¢ KhÃ´ng set requests â†’ Scheduler khÃ´ng biáº¿t pod cáº§n gÃ¬ â†’ Ä‘áº·t sai node
  â€¢ KhÃ´ng set limits â†’ 1 pod Äƒn háº¿t RAM â†’ áº£nh hÆ°á»Ÿng pods khÃ¡c trÃªn cÃ¹ng node
```

### 7.2. QoS Classes

```
QoS (Quality of Service) CLASSES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  K8s phÃ¢n loáº¡i pods thÃ nh 3 QoS class dá»±a trÃªn requests/limits:

  1. Guaranteed â€” requests = limits (cho cáº£ CPU vÃ  Memory)
     resources:
       requests: { cpu: "500m", memory: "512Mi" }
       limits:   { cpu: "500m", memory: "512Mi" }
     â†’ Æ¯u tiÃªn cao nháº¥t. Bá»‹ kill CUá»I CÃ™NG khi node háº¿t resources.
     â†’ DÃ¹ng cho: critical services (payment, database)

  2. Burstable â€” requests < limits (hoáº·c chá»‰ set 1 trong 2)
     resources:
       requests: { cpu: "250m", memory: "256Mi" }
       limits:   { cpu: "500m", memory: "512Mi" }
     â†’ Æ¯u tiÃªn trung bÃ¬nh. Bá»‹ kill sau BestEffort.
     â†’ DÃ¹ng cho: háº§u háº¿t services

  3. BestEffort â€” KHÃ”NG set requests hoáº·c limits
     resources: {}
     â†’ Æ¯u tiÃªn tháº¥p nháº¥t. Bá»‹ kill Äáº¦U TIÃŠN khi node háº¿t resources.
     â†’ DÃ¹ng cho: batch jobs, dev/test

  Khi node háº¿t resources â†’ K8s evict (kill) theo thá»© tá»±:
  BestEffort â†’ Burstable â†’ Guaranteed
```

### 7.3. LimitRange & ResourceQuota

```yaml
# LimitRange â€” Set default vÃ  giá»›i háº¡n cho Má»–I pod/container trong namespace
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: production
spec:
  limits:
    - type: Container
      default:                  # Default limits (náº¿u pod khÃ´ng set)
        cpu: "500m"
        memory: "512Mi"
      defaultRequest:           # Default requests
        cpu: "250m"
        memory: "256Mi"
      max:                      # Max cho 1 container
        cpu: "2"
        memory: "2Gi"
      min:                      # Min cho 1 container
        cpu: "100m"
        memory: "128Mi"
```

```yaml
# ResourceQuota â€” Giá»›i háº¡n Tá»”NG resources cho namespace
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    requests.cpu: "20"          # Tá»•ng CPU requests: 20 cores
    requests.memory: "40Gi"     # Tá»•ng RAM requests: 40 GiB
    limits.cpu: "40"            # Tá»•ng CPU limits: 40 cores
    limits.memory: "80Gi"       # Tá»•ng RAM limits: 80 GiB
    pods: "100"                 # Tá»‘i Ä‘a 100 pods
    services: "20"              # Tá»‘i Ä‘a 20 services
```

---

## 8. Helm â€” Package Manager cho Kubernetes

### 8.1. Helm lÃ  gÃ¬?

Helm lÃ  **package manager cho Kubernetes** â€” giá»‘ng apt cho Ubuntu, brew cho macOS. Thay vÃ¬ quáº£n lÃ½ hÃ ng chá»¥c YAML files â†’ Helm Ä‘Ã³ng gÃ³i thÃ nh 1 **Chart**.

```
Váº¤N Äá»€ KHÃ”NG CÃ“ HELM:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Deploy order-service cáº§n:
  â€¢ deployment.yaml
  â€¢ service.yaml
  â€¢ configmap.yaml
  â€¢ secret.yaml
  â€¢ hpa.yaml
  â€¢ ingress.yaml
  â€¢ serviceaccount.yaml

  Ã— 10 services = 70 YAML files ğŸ˜±
  
  Staging vs Production: háº§u háº¿t giá»‘ng nhau, chá»‰ khÃ¡c
  replicas, image tag, resources, config â†’ duplicate 70 files?

  Helm giáº£i quyáº¿t:
  â†’ 1 Chart template + values.yaml (per environment)
  â†’ helm install order-service ./chart -f values-prod.yaml
```

### 8.2. Helm Chart Structure

```
HELM CHART STRUCTURE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  order-service-chart/
  â”œâ”€â”€ Chart.yaml              # Metadata (name, version, description)
  â”œâ”€â”€ values.yaml             # Default values
  â”œâ”€â”€ values-staging.yaml     # Override cho staging
  â”œâ”€â”€ values-production.yaml  # Override cho production
  â”œâ”€â”€ templates/
  â”‚   â”œâ”€â”€ deployment.yaml     # Deployment template
  â”‚   â”œâ”€â”€ service.yaml        # Service template
  â”‚   â”œâ”€â”€ configmap.yaml      # ConfigMap template
  â”‚   â”œâ”€â”€ hpa.yaml            # HPA template
  â”‚   â”œâ”€â”€ ingress.yaml        # Ingress template
  â”‚   â”œâ”€â”€ _helpers.tpl        # Template helpers
  â”‚   â””â”€â”€ NOTES.txt           # Post-install notes
  â””â”€â”€ charts/                 # Sub-charts (dependencies)
```

### 8.3. Helm Commands

```bash
# Install chart
helm install order-service ./order-service-chart \
  -f values-production.yaml \
  -n production

# Upgrade (deploy version má»›i)
helm upgrade order-service ./order-service-chart \
  -f values-production.yaml \
  --set image.tag=2.1.0 \
  -n production

# Rollback
helm rollback order-service 1 -n production    # Rollback vá» revision 1

# Uninstall
helm uninstall order-service -n production

# List releases
helm list -n production

# Xem rendered YAML (debug)
helm template order-service ./order-service-chart -f values-production.yaml
```

### 8.4. VÃ­ dá»¥ â€” Helm Chart cho Microservice

```yaml
# Chart.yaml
apiVersion: v2
name: order-service
description: Order Service for E-Commerce platform
version: 1.0.0          # Chart version
appVersion: "2.1.0"      # Application version
```

```yaml
# values.yaml â€” Default values
replicaCount: 2

image:
  repository: 123456789.dkr.ecr.ap-southeast-1.amazonaws.com/order-service
  tag: "latest"
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 8082

ingress:
  enabled: false
  host: ""
  path: /orders

resources:
  requests:
    cpu: 250m
    memory: 256Mi
  limits:
    cpu: 500m
    memory: 512Mi

autoscaling:
  enabled: false
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilization: 70

config:
  dbHost: postgres
  dbPort: "5432"
  kafkaBrokers: kafka:9092
  logLevel: INFO

env: {}
```

```yaml
# values-production.yaml â€” Override cho production
replicaCount: 3

image:
  tag: "2.1.0"

ingress:
  enabled: true
  host: api.myapp.com
  path: /orders

resources:
  requests:
    cpu: 500m
    memory: 512Mi
  limits:
    cpu: "1"
    memory: 1Gi

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 20
  targetCPUUtilization: 70

config:
  dbHost: postgres.production.svc
  logLevel: WARN
```

```yaml
# templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "order-service.fullname" . }}
  labels:
    {{- include "order-service.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "order-service.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "order-service.selectorLabels" . | nindent 8 }}
    spec:
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - containerPort: {{ .Values.service.port }}
          env:
            - name: DB_HOST
              valueFrom:
                configMapKeyRef:
                  name: {{ include "order-service.fullname" . }}-config
                  key: db-host
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ include "order-service.fullname" . }}-secrets
                  key: db-password
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          livenessProbe:
            httpGet:
              path: /actuator/health/liveness
              port: {{ .Values.service.port }}
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /actuator/health/readiness
              port: {{ .Values.service.port }}
            initialDelaySeconds: 10
            periodSeconds: 5
```

---

## 9. Service Mesh

### 9.1. Táº¡i sao cáº§n Service Mesh?

```
Váº¤N Äá»€: CROSS-CUTTING CONCERNS TRONG SERVICE-TO-SERVICE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Má»—i service cáº§n xá»­ lÃ½:
  â€¢ mTLS (mutual TLS) â€” mÃ£ hÃ³a traffic giá»¯a services
  â€¢ Retry + Circuit Breaker â€” resilience (xem doc 10)
  â€¢ Load Balancing â€” client-side LB giá»¯a pods
  â€¢ Observability â€” metrics, tracing, logging
  â€¢ Traffic Management â€” canary deploy, A/B testing
  â€¢ Access Control â€” service A Ä‘Æ°á»£c gá»i service B khÃ´ng?

  KhÃ´ng cÃ³ Service Mesh:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Má»—i service pháº£i tá»± implement:                      â”‚
  â”‚                                                      â”‚
  â”‚  Order Service (Java):                               â”‚
  â”‚    + Spring Cloud Circuit Breaker (Resilience4j)     â”‚
  â”‚    + Spring Cloud LoadBalancer                       â”‚
  â”‚    + Micrometer metrics                              â”‚
  â”‚    + OpenTelemetry tracing                           â”‚
  â”‚    + TLS certificates management                     â”‚
  â”‚                                                      â”‚
  â”‚  Product Service (Node.js):                          â”‚
  â”‚    + opossum (circuit breaker)                       â”‚
  â”‚    + client-side LB (custom)                         â”‚
  â”‚    + prom-client (metrics)                           â”‚
  â”‚    + @opentelemetry/sdk (tracing)                    â”‚
  â”‚    + TLS (manual cert management)                    â”‚
  â”‚                                                      â”‚
  â”‚  Payment Service (Go):                               â”‚
  â”‚    + sony/gobreaker (circuit breaker)                â”‚
  â”‚    + grpc-go LB                                      â”‚
  â”‚    + prometheus/client_golang                        â”‚
  â”‚    + go.opentelemetry.io/otel                        â”‚
  â”‚    + TLS                                             â”‚
  â”‚                                                      â”‚
  â”‚  â†’ Má»—i ngÃ´n ngá»¯ = libraries khÃ¡c nhau                â”‚
  â”‚  â†’ Má»—i team implement khÃ¡c nhau                      â”‚
  â”‚  â†’ KhÃ´ng consistent, khÃ³ maintain                    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.2. Service Mesh Architecture

```
SERVICE MESH â€” SIDECAR PROXY PATTERN:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Service Mesh giáº£i quyáº¿t báº±ng cÃ¡ch inject SIDECAR PROXY
  vÃ o má»—i pod. Proxy xá»­ lÃ½ Táº¤T Cáº¢ networking concerns.
  Service code KHÃ”NG Cáº¦N thay Ä‘á»•i.

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Kubernetes Cluster                                      â”‚
  â”‚                                                          â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚  Pod: Order Service    â”‚  â”‚  Pod: Payment Service  â”‚  â”‚
  â”‚  â”‚                        â”‚  â”‚                        â”‚  â”‚
  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
  â”‚  â”‚  â”‚  Order Service   â”‚  â”‚  â”‚  â”‚ Payment Service  â”‚  â”‚  â”‚
  â”‚  â”‚  â”‚  (business code) â”‚  â”‚  â”‚  â”‚ (business code)  â”‚  â”‚  â”‚
  â”‚  â”‚  â”‚  localhost:8082  â”‚  â”‚  â”‚  â”‚ localhost:8084   â”‚  â”‚  â”‚
  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
  â”‚  â”‚           â”‚            â”‚  â”‚           â”‚            â”‚  â”‚
  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
  â”‚  â”‚  â”‚  Sidecar Proxy   â”‚  â”‚  â”‚  â”‚  Sidecar Proxy   â”‚  â”‚  â”‚
  â”‚  â”‚  â”‚  (Envoy/Linkerd) â”‚â—„â”€â”¼â”€â”€â”¼â”€â–¶â”‚  (Envoy/Linkerd) â”‚  â”‚  â”‚
  â”‚  â”‚  â”‚                  â”‚  â”‚  â”‚  â”‚                  â”‚  â”‚  â”‚
  â”‚  â”‚  â”‚  â€¢ mTLS          â”‚  â”‚  â”‚  â”‚  â€¢ mTLS          â”‚  â”‚  â”‚
  â”‚  â”‚  â”‚  â€¢ Retry         â”‚  â”‚  â”‚  â”‚  â€¢ Retry         â”‚  â”‚  â”‚
  â”‚  â”‚  â”‚  â€¢ Circuit Break â”‚  â”‚  â”‚  â”‚  â€¢ Circuit Break â”‚  â”‚  â”‚
  â”‚  â”‚  â”‚  â€¢ Metrics       â”‚  â”‚  â”‚  â”‚  â€¢ Metrics       â”‚  â”‚  â”‚
  â”‚  â”‚  â”‚  â€¢ Tracing       â”‚  â”‚  â”‚  â”‚  â€¢ Tracing       â”‚  â”‚  â”‚
  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â”‚                                                          â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚  Control Plane (Istiod / Linkerd control plane)    â”‚  â”‚
  â”‚  â”‚                                                    â”‚  â”‚
  â”‚  â”‚  â€¢ Push config Ä‘áº¿n proxies                         â”‚  â”‚
  â”‚  â”‚  â€¢ Manage certificates (mTLS auto-rotation)        â”‚  â”‚
  â”‚  â”‚  â€¢ Collect metrics/traces                          â”‚  â”‚
  â”‚  â”‚  â€¢ Traffic policies                                â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  App code GIá»® NGUYÃŠN â€” chá»‰ cáº§n gá»i http://payment-service:8084
  Sidecar proxy tá»± Ä‘á»™ng: encrypt (mTLS), retry, circuit break,
  collect metrics, add tracing headers
```

### 9.3. Istio

```
ISTIO:
â”€â”€â”€â”€â”€â”€

  Phá»• biáº¿n nháº¥t, Ä‘áº§y Ä‘á»§ tÃ­nh nÄƒng nháº¥t.
  Data Plane: Envoy proxy (sidecar)
  Control Plane: Istiod

  TÃ­nh nÄƒng chÃ­nh:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                        â”‚
  â”‚  Traffic Management:                                   â”‚
  â”‚  â€¢ VirtualService â€” routing rules (canary, A/B)        â”‚
  â”‚  â€¢ DestinationRule â€” LB policy, circuit breaker        â”‚
  â”‚  â€¢ Gateway â€” ingress/egress traffic                    â”‚
  â”‚                                                        â”‚
  â”‚  Security:                                             â”‚
  â”‚  â€¢ mTLS â€” automatic, zero-config                       â”‚
  â”‚  â€¢ AuthorizationPolicy â€” service-to-service ACL        â”‚
  â”‚  â€¢ RequestAuthentication â€” JWT validation              â”‚
  â”‚                                                        â”‚
  â”‚  Observability:                                        â”‚
  â”‚  â€¢ Metrics â†’ Prometheus (auto-generated)               â”‚
  â”‚  â€¢ Tracing â†’ Jaeger/Zipkin (auto-propagated)           â”‚
  â”‚  â€¢ Kiali â€” Service Mesh dashboard                      â”‚
  â”‚                                                        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```yaml
# Istio VirtualService â€” Canary deployment 90/10
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: order-service
spec:
  hosts:
    - order-service
  http:
    - route:
        - destination:
            host: order-service
            subset: stable
          weight: 90              # 90% traffic â†’ v1.0
        - destination:
            host: order-service
            subset: canary
          weight: 10              # 10% traffic â†’ v2.0

---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: order-service
spec:
  host: order-service
  subsets:
    - name: stable
      labels:
        version: "1.0"
    - name: canary
      labels:
        version: "2.0"
  trafficPolicy:
    connectionPool:
      http:
        maxConnections: 100
    outlierDetection:               # Circuit Breaker
      consecutive5xxErrors: 5
      interval: 30s
      baseEjectionTime: 30s
```

### 9.4. Linkerd

```
LINKERD:
â”€â”€â”€â”€â”€â”€â”€â”€

  Lightweight, dá»… cÃ i Ä‘áº·t, focus vÃ o simplicity.
  Data Plane: linkerd2-proxy (Rust â€” nháº¹ hÆ¡n Envoy)
  Control Plane: Linkerd control plane

  So vá»›i Istio:
  â€¢ CÃ i Ä‘áº·t: linkerd install | kubectl apply (2 phÃºt)
  â€¢ Resource: ~50% Ã­t hÆ¡n Istio
  â€¢ TÃ­nh nÄƒng: Ã­t hÆ¡n Istio nhÆ°ng Ä‘á»§ cho Ä‘a sá»‘ use cases
  â€¢ mTLS: automatic (giá»‘ng Istio)
  â€¢ Traffic split: cÃ³ (nhÆ°ng Ã­t flexible hÆ¡n Istio)
  â€¢ Multi-cluster: cÃ³
```

### 9.5. So sÃ¡nh Istio vs Linkerd

| TiÃªu chÃ­ | Istio | Linkerd |
|----------|-------|---------|
| **Proxy** | Envoy (C++) | linkerd2-proxy (Rust) |
| **Resource usage** | Cao (~128MB/sidecar) | Tháº¥p (~20MB/sidecar) |
| **Complexity** | Cao (nhiá»u CRDs, config) | Tháº¥p (simple, opinionated) |
| **Features** | Ráº¥t Ä‘áº§y Ä‘á»§ (traffic mgmt, security, observability) | Äá»§ dÃ¹ng (mTLS, metrics, traffic split) |
| **Traffic Management** | Ráº¥t flexible (VirtualService, DestinationRule) | Basic (TrafficSplit, ServiceProfile) |
| **mTLS** | Auto, cÃ³ thá»ƒ customize | Auto, zero-config |
| **Multi-cluster** | CÃ³ (phá»©c táº¡p) | CÃ³ (Ä‘Æ¡n giáº£n hÆ¡n) |
| **Learning curve** | Dá»‘c | Thoai |
| **Community** | Lá»›n (Google, IBM) | Trung bÃ¬nh (Buoyant, CNCF graduated) |
| **PhÃ¹ há»£p** | Enterprise, cáº§n fine-grained control | Startup/mid-size, cáº§n simple mesh |

### 9.6. Khi nÃ o cáº§n / khÃ´ng cáº§n Service Mesh?

| Cáº§n Service Mesh | KhÃ´ng cáº§n Service Mesh |
|------------------|----------------------|
| > 10 services, polyglot (nhiá»u ngÃ´n ngá»¯) | < 5 services, cÃ¹ng ngÃ´n ngá»¯ |
| Cáº§n mTLS giá»¯a táº¥t cáº£ services | Internal network trusted |
| Cáº§n canary/A/B testing á»Ÿ network level | Simple rolling update Ä‘á»§ dÃ¹ng |
| Cáº§n fine-grained traffic control | Basic routing Ä‘á»§ |
| Compliance yÃªu cáº§u encryption in-transit | KhÃ´ng cÃ³ compliance requirement |
| Team cÃ³ kinh nghiá»‡m K8s | Team má»›i dÃ¹ng K8s (thÃªm mesh = quÃ¡ phá»©c táº¡p) |

> **Thá»±c táº¿**: Äa sá»‘ startup KHÃ”NG cáº§n Service Mesh giai Ä‘oáº¡n Ä‘áº§u. Báº¯t Ä‘áº§u vá»›i K8s thuáº§n â†’ khi há»‡ thá»‘ng phá»©c táº¡p (>10 services, security requirements) â†’ thÃªm Linkerd (Ä‘Æ¡n giáº£n) hoáº·c Istio (Ä‘áº§y Ä‘á»§).

---

## 10. Kubernetes trÃªn Cloud

### 10.1. Managed Kubernetes Services

| Service | Provider | Äáº·c Ä‘iá»ƒm |
|---------|----------|-----------|
| **Amazon EKS** | AWS | TÃ­ch há»£p ALB, ECR, IAM, CloudWatch. Fargate (serverless pods) |
| **Google GKE** | GCP | K8s gá»‘c (Google táº¡o K8s). Autopilot mode (fully managed nodes) |
| **Azure AKS** | Azure | TÃ­ch há»£p Azure AD, ACR, Azure Monitor. Free control plane |
| **DigitalOcean DOKS** | DigitalOcean | ÄÆ¡n giáº£n, giÃ¡ ráº», phÃ¹ há»£p startup |

### 10.2. Self-managed vs Managed

```
SELF-MANAGED vs MANAGED KUBERNETES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Self-managed (kubeadm, kops, Rancher):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Báº¡n quáº£n lÃ½:                           â”‚
  â”‚  âœ— Control Plane (API Server, etcd...)  â”‚
  â”‚  âœ— Worker Nodes                         â”‚
  â”‚  âœ— Networking (CNI)                     â”‚
  â”‚  âœ— Upgrades (K8s version)               â”‚
  â”‚  âœ— etcd backup                          â”‚
  â”‚  âœ— HA cho control plane                 â”‚
  â”‚                                         â”‚
  â”‚  â†’ Cáº§n DevOps team giá»i                 â”‚
  â”‚  â†’ Full control nhÆ°ng tá»‘n effort        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Managed (EKS, GKE, AKS):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Cloud quáº£n lÃ½:                         â”‚
  â”‚  âœ… Control Plane (HA, auto-upgrade)    â”‚
  â”‚  âœ… etcd (managed, backed up)           â”‚
  â”‚  âœ… K8s upgrades                        â”‚
  â”‚  âœ… Security patches                    â”‚
  â”‚                                         â”‚
  â”‚  Báº¡n quáº£n lÃ½:                           â”‚
  â”‚  âœ— Worker Nodes (hoáº·c dÃ¹ng Fargate)     â”‚
  â”‚  âœ— Workloads (Deployments, Services)    â”‚
  â”‚  âœ— Monitoring, Logging                  â”‚
  â”‚                                         â”‚
  â”‚  â†’ Nhanh, Ã­t ops, táº­p trung vÃ o app     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â†’ 90%+ companies chá»n Managed Kubernetes
```

---

## 11. VÃ­ dá»¥ thá»±c táº¿ â€” E-Commerce trÃªn Kubernetes

```
E-COMMERCE ON KUBERNETES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Scenario: 6 microservices trÃªn AWS EKS

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  AWS EKS Cluster (Managed Kubernetes)                      â”‚
  â”‚                                                            â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚  Control Plane (AWS managed)                         â”‚  â”‚
  â”‚  â”‚  API Server, etcd, Scheduler, Controller Manager     â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â”‚                                                            â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚  Namespace: ingress                                  â”‚  â”‚
  â”‚  â”‚                                                      â”‚  â”‚
  â”‚  â”‚  ALB (AWS) â†’ NGINX Ingress Controller (3 pods)       â”‚  â”‚
  â”‚  â”‚              /users   â†’ user-service                 â”‚  â”‚
  â”‚  â”‚              /orders  â†’ order-service                â”‚  â”‚
  â”‚  â”‚              /productsâ†’ product-service              â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â”‚                                                            â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚  Namespace: production                               â”‚  â”‚
  â”‚  â”‚                                                      â”‚  â”‚
  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚  â”‚
  â”‚  â”‚  â”‚user-serviceâ”‚ â”‚order-serviceâ”‚â”‚product-svc â”‚        â”‚  â”‚
  â”‚  â”‚  â”‚ Deployment â”‚ â”‚ Deployment â”‚ â”‚ Deployment â”‚        â”‚  â”‚
  â”‚  â”‚  â”‚ 3 pods     â”‚ â”‚ 5 pods     â”‚ â”‚ 3 pods     â”‚        â”‚  â”‚
  â”‚  â”‚  â”‚ HPA: 3-10  â”‚ â”‚ HPA: 5-20  â”‚ â”‚ HPA: 3-10  â”‚        â”‚  â”‚
  â”‚  â”‚  â”‚ Java 21    â”‚ â”‚ Java 21    â”‚ â”‚ Node.js 20 â”‚        â”‚  â”‚
  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  â”‚
  â”‚  â”‚                                                      â”‚  â”‚
  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚  â”‚
  â”‚  â”‚  â”‚payment-svc â”‚ â”‚notificationâ”‚ â”‚inventory   â”‚        â”‚  â”‚
  â”‚  â”‚  â”‚ Deployment â”‚ â”‚ Deployment â”‚ â”‚ Deployment â”‚        â”‚  â”‚
  â”‚  â”‚  â”‚ 3 pods     â”‚ â”‚ 2 pods     â”‚ â”‚ 3 pods     â”‚        â”‚  â”‚
  â”‚  â”‚  â”‚ HPA: 3-10  â”‚ â”‚ KEDA:0-5   â”‚ â”‚ HPA: 3-10  â”‚        â”‚  â”‚
  â”‚  â”‚  â”‚ Go 1.22    â”‚ â”‚ Python 3.12â”‚ â”‚ Java 21    â”‚        â”‚  â”‚
  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â”‚                                                            â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚  Namespace: data                                     â”‚  â”‚
  â”‚  â”‚                                                      â”‚  â”‚
  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚  â”‚
  â”‚  â”‚  â”‚ PostgreSQL â”‚ â”‚   Redis    â”‚ â”‚   Kafka    â”‚        â”‚  â”‚
  â”‚  â”‚  â”‚ StatefulSetâ”‚ â”‚ StatefulSetâ”‚ â”‚ StatefulSetâ”‚        â”‚  â”‚
  â”‚  â”‚  â”‚ 3 replicas â”‚ â”‚ 3 replicas â”‚ â”‚ 3 brokers  â”‚        â”‚  â”‚
  â”‚  â”‚  â”‚ PVC: 100Gi â”‚ â”‚ PVC: 10Gi  â”‚ â”‚ PVC: 50Gi  â”‚        â”‚  â”‚
  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â”‚                                                            â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚  Namespace: monitoring                               â”‚  â”‚
  â”‚  â”‚                                                      â”‚  â”‚
  â”‚  â”‚  Prometheus + Grafana + Jaeger + Fluent Bit          â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â”‚                                                            â”‚
  â”‚  Worker Nodes: 5 Ã— m5.xlarge (4 vCPU, 16 GB RAM)           â”‚
  â”‚  Cluster Autoscaler: 3 â†’ 10 nodes                          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Deploy Flow:
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Developer â†’ git push â†’ GitHub Actions:
    1. docker build (multi-stage)
    2. docker push â†’ ECR
    3. helm upgrade order-service \
         --set image.tag=${GIT_SHA} \
         -f values-production.yaml
    4. K8s rolling update â†’ zero downtime
```

---

## 12. Anti-patterns

| Anti-pattern | MÃ´ táº£ | Triá»‡u chá»©ng | CÃ¡ch fix |
|-------------|--------|-------------|----------|
| **No resource limits** | KhÃ´ng set requests/limits | 1 pod Äƒn háº¿t RAM node â†’ cÃ¡c pods khÃ¡c bá»‹ OOMKilled | LuÃ´n set requests vÃ  limits |
| **Latest tag** | `image: service:latest` | KhÃ´ng biáº¿t version nÃ o Ä‘ang cháº¡y, cache image cÅ© | Pin version: `service:2.1.0` hoáº·c git SHA |
| **Single replica** | replicas: 1 trong production | Pod restart â†’ downtime, khÃ´ng HA | Tá»‘i thiá»ƒu 2-3 replicas cho critical services |
| **No health probes** | KhÃ´ng cÃ³ liveness/readiness probe | Pod cháº¡y nhÆ°ng app crash â†’ váº«n nháº­n traffic | Implement health endpoints + probes |
| **Kubectl apply trá»±c tiáº¿p** | Edit YAML thá»§ cÃ´ng, `kubectl apply` | KhÃ´ng track version, khÃ³ rollback, khÃ´ng reproducible | DÃ¹ng Helm/Kustomize + GitOps (ArgoCD) |
| **Secrets in ConfigMap** | ÄÆ°a password vÃ o ConfigMap (plain text) | Báº¥t ká»³ ai cÃ³ access namespace Ä‘á»u tháº¥y | DÃ¹ng Secret + encryption at rest + External Secrets |
| **Pod trá»±c tiáº¿p** | Táº¡o Pod mÃ  khÃ´ng dÃ¹ng Deployment | Pod crash â†’ khÃ´ng ai táº¡o láº¡i, khÃ´ng rolling update | LuÃ´n dÃ¹ng Deployment (hoáº·c StatefulSet cho stateful) |
| **No namespace** | Táº¥t cáº£ resources trong `default` namespace | KhÃ´ng phÃ¢n quyá»n Ä‘Æ°á»£c, resource quota khÃ´ng Ã¡p dá»¥ng | Táº¡o namespaces: production, staging, monitoring |
| **Premature Service Mesh** | CÃ i Istio khi má»›i cÃ³ 3 services | Complexity tÄƒng, resource overhead, team khÃ´ng hiá»ƒu | Báº¯t Ä‘áº§u K8s thuáº§n â†’ thÃªm mesh khi thá»±c sá»± cáº§n |

---

## 13. Checklist triá»ƒn khai

```
KUBERNETES CHECKLIST:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Workloads:
  â˜ Deployment cho stateless services
  â˜ StatefulSet cho stateful (DB, Kafka)
  â˜ Resource requests & limits cho má»i container
  â˜ Liveness + Readiness probes
  â˜ Startup probe cho app khá»Ÿi Ä‘á»™ng cháº­m
  â˜ Rolling update strategy (maxSurge, maxUnavailable)
  â˜ Pod Disruption Budget (PDB)

  Networking:
  â˜ Service (ClusterIP) cho internal communication
  â˜ Ingress cho external traffic
  â˜ Network Policies (restrict pod-to-pod traffic)
  â˜ TLS termination táº¡i Ingress

  Config & Secrets:
  â˜ ConfigMap cho non-sensitive config
  â˜ Secret cho sensitive data
  â˜ External Secrets Operator (náº¿u dÃ¹ng Vault/AWS Secrets Manager)

  Scaling:
  â˜ HPA cho auto-scaling pods
  â˜ Cluster Autoscaler cho auto-scaling nodes
  â˜ KEDA cho event-driven workloads

  Observability:
  â˜ Prometheus + Grafana (metrics)
  â˜ Centralized logging (Fluent Bit â†’ CloudWatch/ELK)
  â˜ Distributed tracing (Jaeger/Tempo)

  Operations:
  â˜ Helm charts cho má»—i service
  â˜ GitOps (ArgoCD/Flux) cho deployment
  â˜ Namespaces: production, staging, monitoring, data
  â˜ RBAC (Role-Based Access Control)
  â˜ ResourceQuota per namespace
  â˜ Backup etcd (managed K8s: tá»± Ä‘á»™ng)
```

---

## 14. Tá»•ng káº¿t

```
ORCHESTRATION â€” Tá»”NG Káº¾T:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Kubernetes = Standard cho container orchestration
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                      â”‚
  â”‚  Scheduling:    K8s chá»n node â†’ Ä‘áº·t pod              â”‚
  â”‚  Self-healing:  Pod crash â†’ tá»± restart/táº¡o má»›i       â”‚
  â”‚  Scaling:       HPA (pods) + Cluster Autoscaler      â”‚
  â”‚                 (nodes) â†’ auto-scale                 â”‚
  â”‚  Rolling Update: Zero downtime deployment            â”‚
  â”‚  Service Discovery: DNS + ClusterIP (built-in)       â”‚
  â”‚  Config:        ConfigMap + Secret                   â”‚
  â”‚                                                      â”‚
  â”‚  Deployment Flow:                                    â”‚
  â”‚  Code â†’ Docker Image â†’ Helm Chart â†’ K8s Cluster      â”‚
  â”‚                                                      â”‚
  â”‚  Service Mesh (optional):                            â”‚
  â”‚  Istio/Linkerd â†’ mTLS, traffic management,           â”‚
  â”‚  observability á»Ÿ infrastructure level                â”‚
  â”‚                                                      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Chá»n nhanh:
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Managed K8s (EKS/GKE/AKS)?     â†’ 90%+ chá»n managed
  Package Manager?                 â†’ Helm
  GitOps?                          â†’ ArgoCD hoáº·c Flux
  Service Mesh?                    â†’ ChÆ°a cáº§n â†’ K8s thuáº§n
                                   â†’ Cáº§n simple â†’ Linkerd
                                   â†’ Cáº§n full â†’ Istio
```

```mermaid
graph LR
    DEV[Developer] -->|git push| CI[CI/CD]
    CI -->|docker build| IMG[Docker Image]
    IMG -->|push| ECR[Container Registry]
    CI -->|helm upgrade| K8S[Kubernetes Cluster]
    K8S -->|pull image| ECR
    K8S --> PODS[Running Pods]
    PODS --> HPA[Auto-Scale]
    PODS --> HEAL[Self-Heal]
    PODS --> ROLL[Rolling Update]
```

---

## 15. LiÃªn káº¿t liÃªn quan

| Doc | LiÃªn quan |
|-----|-----------|
| [06 - Inter-Service Communication](06-inter-service-communication.md) | K8s Service cho service-to-service |
| [07 - API Gateway](07-api-gateway.md) | Ingress Controller = API Gateway trÃªn K8s |
| [08 - Service Discovery](08-service-discovery.md) | K8s DNS, CoreDNS, ClusterIP |
| [09 - Data Management](09-data-management.md) | StatefulSet, PersistentVolume cho databases |
| [10 - Resilience Patterns](10-resilience-patterns.md) | Health probes, Service Mesh circuit breaker |
| [11 - Observability](11-observability-evolvability.md) | Prometheus, Grafana, Jaeger trÃªn K8s |
| [12 - Containerization](12-containerization.md) | Docker image â†’ K8s cháº¡y containers |
| [14 - CI/CD & Deployment](14-cicd-deployment.md) | Helm, ArgoCD, deployment strategies |
| [15 - Security](15-security.md) | mTLS (Service Mesh), RBAC, Network Policies |
| [17 - Configuration Management](17-configuration-secrets-management.md) | ConfigMap, Secret, External Secrets |
